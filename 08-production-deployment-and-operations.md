# ç¬¬å…«ç« ï¼šç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ä¸è¿ç»´

## æœ¬ç« æ¦‚è¿°

æœ¬ç« å°†æ·±å…¥æ¢è®¨ SQLModel åº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€é…ç½®ã€ç›‘æ§å’Œè¿ç»´æœ€ä½³å®è·µã€‚æˆ‘ä»¬å°†æ¶µç›–ä»å¼€å‘ç¯å¢ƒåˆ°ç”Ÿäº§ç¯å¢ƒçš„å®Œæ•´éƒ¨ç½²æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®åº“é…ç½®ä¼˜åŒ–ã€å®‰å…¨æ€§è®¾ç½®ã€æ€§èƒ½ç›‘æ§ã€æ•…éšœæ’é™¤å’Œè‡ªåŠ¨åŒ–è¿ç»´ç­‰å…³é”®ä¸»é¢˜ã€‚

### å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- æŒæ¡ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“é…ç½®å’Œä¼˜åŒ–æŠ€å·§
- å®æ–½å…¨é¢çš„å®‰å…¨æ€§æªæ–½å’Œè®¿é—®æ§åˆ¶
- å»ºç«‹å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»
- å®ç°è‡ªåŠ¨åŒ–éƒ¨ç½²å’Œè¿ç»´æµç¨‹
- å¤„ç†å¸¸è§çš„ç”Ÿäº§ç¯å¢ƒé—®é¢˜å’Œæ•…éšœ
- åˆ¶å®šæœ‰æ•ˆçš„å¤‡ä»½å’Œç¾éš¾æ¢å¤ç­–ç•¥

---

## 1. ç”Ÿäº§ç¯å¢ƒé…ç½®

### 1.1 æ•°æ®åº“è¿æ¥é…ç½®

```python
# production_config.py
from sqlmodel import SQLModel, create_engine
from sqlalchemy.pool import QueuePool
from sqlalchemy import event
from typing import Optional
import os
import logging
from urllib.parse import quote_plus

class ProductionDatabaseConfig:
    """ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“é…ç½®"""
    
    def __init__(self):
        self.database_url = self._build_database_url()
        self.engine = self._create_engine()
        self._setup_event_listeners()
    
    def _build_database_url(self) -> str:
        """æ„å»ºæ•°æ®åº“è¿æ¥URL"""
        # ä»ç¯å¢ƒå˜é‡è·å–æ•°æ®åº“é…ç½®
        db_host = os.getenv('DB_HOST', 'localhost')
        db_port = os.getenv('DB_PORT', '5432')
        db_name = os.getenv('DB_NAME', 'production_db')
        db_user = os.getenv('DB_USER', 'app_user')
        db_password = os.getenv('DB_PASSWORD', '')
        
        # URLç¼–ç å¯†ç ä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦
        encoded_password = quote_plus(db_password)
        
        # æ„å»ºè¿æ¥URL
        database_url = (
            f"postgresql://{db_user}:{encoded_password}"
            f"@{db_host}:{db_port}/{db_name}"
        )
        
        # æ·»åŠ SSLé…ç½®
        ssl_mode = os.getenv('DB_SSL_MODE', 'require')
        if ssl_mode:
            database_url += f"?sslmode={ssl_mode}"
        
        return database_url
    
    def _create_engine(self):
        """åˆ›å»ºæ•°æ®åº“å¼•æ“"""
        return create_engine(
            self.database_url,
            # è¿æ¥æ± é…ç½®
            poolclass=QueuePool,
            pool_size=int(os.getenv('DB_POOL_SIZE', '20')),
            max_overflow=int(os.getenv('DB_MAX_OVERFLOW', '30')),
            pool_timeout=int(os.getenv('DB_POOL_TIMEOUT', '30')),
            pool_recycle=int(os.getenv('DB_POOL_RECYCLE', '3600')),
            pool_pre_ping=True,  # è¿æ¥å¥åº·æ£€æŸ¥
            
            # æŸ¥è¯¢é…ç½®
            echo=False,  # ç”Ÿäº§ç¯å¢ƒå…³é—­SQLæ—¥å¿—
            echo_pool=False,
            
            # è¿æ¥å‚æ•°
            connect_args={
                "connect_timeout": 10,
                "command_timeout": 60,
                "server_settings": {
                    "application_name": os.getenv('APP_NAME', 'SQLModel_App'),
                    "timezone": "UTC"
                }
            }
        )
    
    def _setup_event_listeners(self):
        """è®¾ç½®äº‹ä»¶ç›‘å¬å™¨"""
        @event.listens_for(self.engine, "connect")
        def set_sqlite_pragma(dbapi_connection, connection_record):
            """è¿æ¥æ—¶è®¾ç½®æ•°æ®åº“å‚æ•°"""
            if 'postgresql' in self.database_url:
                with dbapi_connection.cursor() as cursor:
                    # è®¾ç½®æŸ¥è¯¢è¶…æ—¶
                    cursor.execute("SET statement_timeout = '60s'")
                    # è®¾ç½®é”ç­‰å¾…è¶…æ—¶
                    cursor.execute("SET lock_timeout = '30s'")
                    # è®¾ç½®ç©ºé—²è¿æ¥è¶…æ—¶
                    cursor.execute("SET idle_in_transaction_session_timeout = '300s'")
        
        @event.listens_for(self.engine, "checkout")
        def receive_checkout(dbapi_connection, connection_record, connection_proxy):
            """è¿æ¥æ£€å‡ºæ—¶çš„å¤„ç†"""
            logging.debug(f"è¿æ¥æ£€å‡º: {id(dbapi_connection)}")
        
        @event.listens_for(self.engine, "checkin")
        def receive_checkin(dbapi_connection, connection_record):
            """è¿æ¥å½’è¿˜æ—¶çš„å¤„ç†"""
            logging.debug(f"è¿æ¥å½’è¿˜: {id(dbapi_connection)}")
    
    def get_engine(self):
        """è·å–æ•°æ®åº“å¼•æ“"""
        return self.engine
    
    def test_connection(self) -> bool:
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute("SELECT 1")
                return result.scalar() == 1
        except Exception as e:
            logging.error(f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def get_connection_info(self) -> dict:
        """è·å–è¿æ¥ä¿¡æ¯"""
        pool = self.engine.pool
        return {
            "pool_size": pool.size(),
            "checked_in": pool.checkedin(),
            "checked_out": pool.checkedout(),
            "overflow": pool.overflow(),
            "invalid": pool.invalid()
        }

# ç¯å¢ƒé…ç½®ç®¡ç†
class EnvironmentConfig:
    """ç¯å¢ƒé…ç½®ç®¡ç†"""
    
    def __init__(self, env: str = None):
        self.env = env or os.getenv('ENVIRONMENT', 'development')
        self.config = self._load_config()
    
    def _load_config(self) -> dict:
        """åŠ è½½ç¯å¢ƒé…ç½®"""
        base_config = {
            "DEBUG": False,
            "TESTING": False,
            "LOG_LEVEL": "INFO",
            "DB_POOL_SIZE": 20,
            "DB_MAX_OVERFLOW": 30,
            "CACHE_TTL": 3600,
            "RATE_LIMIT": 1000
        }
        
        env_configs = {
            "development": {
                "DEBUG": True,
                "LOG_LEVEL": "DEBUG",
                "DB_POOL_SIZE": 5,
                "DB_MAX_OVERFLOW": 10
            },
            "testing": {
                "TESTING": True,
                "LOG_LEVEL": "WARNING",
                "DB_POOL_SIZE": 2,
                "DB_MAX_OVERFLOW": 5
            },
            "staging": {
                "LOG_LEVEL": "INFO",
                "DB_POOL_SIZE": 10,
                "DB_MAX_OVERFLOW": 20
            },
            "production": {
                "LOG_LEVEL": "WARNING",
                "DB_POOL_SIZE": 20,
                "DB_MAX_OVERFLOW": 30
            }
        }
        
        config = base_config.copy()
        config.update(env_configs.get(self.env, {}))
        
        # ä»ç¯å¢ƒå˜é‡è¦†ç›–é…ç½®
        for key, default_value in config.items():
            env_value = os.getenv(key)
            if env_value is not None:
                # ç±»å‹è½¬æ¢
                if isinstance(default_value, bool):
                    config[key] = env_value.lower() in ('true', '1', 'yes')
                elif isinstance(default_value, int):
                    config[key] = int(env_value)
                else:
                    config[key] = env_value
        
        return config
    
    def get(self, key: str, default=None):
        """è·å–é…ç½®å€¼"""
        return self.config.get(key, default)
    
    def is_production(self) -> bool:
        """æ˜¯å¦ä¸ºç”Ÿäº§ç¯å¢ƒ"""
        return self.env == 'production'
    
    def is_development(self) -> bool:
        """æ˜¯å¦ä¸ºå¼€å‘ç¯å¢ƒ"""
        return self.env == 'development'

# ä½¿ç”¨ç¤ºä¾‹
def setup_production_database():
    """è®¾ç½®ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“"""
    # åŠ è½½ç¯å¢ƒé…ç½®
    env_config = EnvironmentConfig()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(
        level=getattr(logging, env_config.get('LOG_LEVEL')),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºæ•°æ®åº“é…ç½®
    db_config = ProductionDatabaseConfig()
    
    # æµ‹è¯•è¿æ¥
    if db_config.test_connection():
        logging.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
        logging.info(f"è¿æ¥ä¿¡æ¯: {db_config.get_connection_info()}")
    else:
        logging.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
        raise RuntimeError("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“")
    
    return db_config.get_engine()
```

### 1.2 å®‰å…¨é…ç½®

```python
# security_config.py
from sqlmodel import SQLModel, Field, Session, select
from sqlalchemy import create_engine, event, text
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
import hashlib
import secrets
import logging
import os
import jwt
from cryptography.fernet import Fernet

class SecurityConfig:
    """å®‰å…¨é…ç½®ç®¡ç†"""
    
    def __init__(self):
        self.secret_key = self._get_secret_key()
        self.jwt_secret = self._get_jwt_secret()
        self.encryption_key = self._get_encryption_key()
        self.cipher_suite = Fernet(self.encryption_key)
    
    def _get_secret_key(self) -> str:
        """è·å–åº”ç”¨å¯†é’¥"""
        secret_key = os.getenv('SECRET_KEY')
        if not secret_key:
            if os.getenv('ENVIRONMENT') == 'production':
                raise ValueError("ç”Ÿäº§ç¯å¢ƒå¿…é¡»è®¾ç½® SECRET_KEY")
            secret_key = secrets.token_urlsafe(32)
            logging.warning("ä½¿ç”¨ä¸´æ—¶ç”Ÿæˆçš„å¯†é’¥ï¼Œè¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è®¾ç½® SECRET_KEY")
        return secret_key
    
    def _get_jwt_secret(self) -> str:
        """è·å–JWTå¯†é’¥"""
        jwt_secret = os.getenv('JWT_SECRET')
        if not jwt_secret:
            if os.getenv('ENVIRONMENT') == 'production':
                raise ValueError("ç”Ÿäº§ç¯å¢ƒå¿…é¡»è®¾ç½® JWT_SECRET")
            jwt_secret = secrets.token_urlsafe(32)
        return jwt_secret
    
    def _get_encryption_key(self) -> bytes:
        """è·å–åŠ å¯†å¯†é’¥"""
        encryption_key = os.getenv('ENCRYPTION_KEY')
        if not encryption_key:
            if os.getenv('ENVIRONMENT') == 'production':
                raise ValueError("ç”Ÿäº§ç¯å¢ƒå¿…é¡»è®¾ç½® ENCRYPTION_KEY")
            encryption_key = Fernet.generate_key().decode()
        return encryption_key.encode()
    
    def encrypt_data(self, data: str) -> str:
        """åŠ å¯†æ•°æ®"""
        return self.cipher_suite.encrypt(data.encode()).decode()
    
    def decrypt_data(self, encrypted_data: str) -> str:
        """è§£å¯†æ•°æ®"""
        return self.cipher_suite.decrypt(encrypted_data.encode()).decode()
    
    def generate_token(self, user_id: int, expires_in: int = 3600) -> str:
        """ç”ŸæˆJWTä»¤ç‰Œ"""
        payload = {
            'user_id': user_id,
            'exp': datetime.utcnow() + timedelta(seconds=expires_in),
            'iat': datetime.utcnow()
        }
        return jwt.encode(payload, self.jwt_secret, algorithm='HS256')
    
    def verify_token(self, token: str) -> Optional[Dict[str, Any]]:
        """éªŒè¯JWTä»¤ç‰Œ"""
        try:
            payload = jwt.decode(token, self.jwt_secret, algorithms=['HS256'])
            return payload
        except jwt.ExpiredSignatureError:
            logging.warning("ä»¤ç‰Œå·²è¿‡æœŸ")
            return None
        except jwt.InvalidTokenError:
            logging.warning("æ— æ•ˆä»¤ç‰Œ")
            return None

# æ•°æ®åº“å®‰å…¨æ¨¡å‹
class AuditLog(SQLModel, table=True):
    """å®¡è®¡æ—¥å¿—æ¨¡å‹"""
    __tablename__ = "audit_logs"
    
    id: Optional[int] = Field(default=None, primary_key=True)
    user_id: Optional[int] = Field(default=None, index=True)
    action: str = Field(max_length=100, index=True)
    table_name: str = Field(max_length=100, index=True)
    record_id: Optional[str] = Field(default=None, index=True)
    old_values: Optional[str] = Field(default=None)  # JSONå­—ç¬¦ä¸²
    new_values: Optional[str] = Field(default=None)  # JSONå­—ç¬¦ä¸²
    ip_address: Optional[str] = Field(default=None, max_length=45)
    user_agent: Optional[str] = Field(default=None, max_length=500)
    timestamp: datetime = Field(default_factory=datetime.utcnow, index=True)
    
class SecurityEvent(SQLModel, table=True):
    """å®‰å…¨äº‹ä»¶æ¨¡å‹"""
    __tablename__ = "security_events"
    
    id: Optional[int] = Field(default=None, primary_key=True)
    event_type: str = Field(max_length=50, index=True)  # login_failed, suspicious_activity, etc.
    severity: str = Field(max_length=20, index=True)  # low, medium, high, critical
    user_id: Optional[int] = Field(default=None, index=True)
    ip_address: Optional[str] = Field(default=None, max_length=45, index=True)
    user_agent: Optional[str] = Field(default=None, max_length=500)
    details: Optional[str] = Field(default=None)  # JSONå­—ç¬¦ä¸²
    timestamp: datetime = Field(default_factory=datetime.utcnow, index=True)
    resolved: bool = Field(default=False, index=True)
    resolved_at: Optional[datetime] = Field(default=None)
    resolved_by: Optional[int] = Field(default=None)

class DatabaseSecurity:
    """æ•°æ®åº“å®‰å…¨ç®¡ç†"""
    
    def __init__(self, engine, security_config: SecurityConfig):
        self.engine = engine
        self.security_config = security_config
        self._setup_audit_triggers()
    
    def _setup_audit_triggers(self):
        """è®¾ç½®å®¡è®¡è§¦å‘å™¨"""
        @event.listens_for(self.engine, "before_cursor_execute")
        def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            # è®°å½•æŸ¥è¯¢å¼€å§‹æ—¶é—´
            context._query_start_time = datetime.utcnow()
            
            # æ£€æŸ¥æ•æ„Ÿæ“ä½œ
            sensitive_operations = ['DROP', 'DELETE', 'UPDATE', 'ALTER', 'TRUNCATE']
            statement_upper = statement.upper().strip()
            
            for operation in sensitive_operations:
                if statement_upper.startswith(operation):
                    logging.warning(f"æ•æ„Ÿæ“ä½œæ£€æµ‹: {operation} - {statement[:100]}...")
                    break
        
        @event.listens_for(self.engine, "after_cursor_execute")
        def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            if hasattr(context, '_query_start_time'):
                execution_time = (datetime.utcnow() - context._query_start_time).total_seconds()
                
                # è®°å½•æ…¢æŸ¥è¯¢
                if execution_time > 5.0:  # 5ç§’ä»¥ä¸Šçš„æŸ¥è¯¢
                    logging.warning(f"æ…¢æŸ¥è¯¢æ£€æµ‹: {execution_time:.3f}s - {statement[:100]}...")
    
    def log_audit_event(self, session: Session, user_id: Optional[int], 
                       action: str, table_name: str, record_id: Optional[str] = None,
                       old_values: Optional[dict] = None, new_values: Optional[dict] = None,
                       ip_address: Optional[str] = None, user_agent: Optional[str] = None):
        """è®°å½•å®¡è®¡äº‹ä»¶"""
        import json
        
        audit_log = AuditLog(
            user_id=user_id,
            action=action,
            table_name=table_name,
            record_id=record_id,
            old_values=json.dumps(old_values) if old_values else None,
            new_values=json.dumps(new_values) if new_values else None,
            ip_address=ip_address,
            user_agent=user_agent
        )
        
        session.add(audit_log)
        session.commit()
    
    def log_security_event(self, session: Session, event_type: str, severity: str,
                          user_id: Optional[int] = None, ip_address: Optional[str] = None,
                          user_agent: Optional[str] = None, details: Optional[dict] = None):
        """è®°å½•å®‰å…¨äº‹ä»¶"""
        import json
        
        security_event = SecurityEvent(
            event_type=event_type,
            severity=severity,
            user_id=user_id,
            ip_address=ip_address,
            user_agent=user_agent,
            details=json.dumps(details) if details else None
        )
        
        session.add(security_event)
        session.commit()
        
        # é«˜ä¸¥é‡æ€§äº‹ä»¶ç«‹å³å‘Šè­¦
        if severity in ['high', 'critical']:
            self._send_security_alert(security_event)
    
    def _send_security_alert(self, event: SecurityEvent):
        """å‘é€å®‰å…¨å‘Šè­¦"""
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶ã€çŸ­ä¿¡ã€Slackç­‰å‘Šè­¦ç³»ç»Ÿ
        logging.critical(f"å®‰å…¨å‘Šè­¦: {event.event_type} - ä¸¥é‡æ€§: {event.severity}")
    
    def check_suspicious_activity(self, session: Session, user_id: int, 
                                 ip_address: str, time_window: int = 300) -> bool:
        """æ£€æŸ¥å¯ç–‘æ´»åŠ¨"""
        # æ£€æŸ¥çŸ­æ—¶é—´å†…çš„å¤šæ¬¡å¤±è´¥ç™»å½•
        cutoff_time = datetime.utcnow() - timedelta(seconds=time_window)
        
        failed_attempts = session.exec(
            select(SecurityEvent)
            .where(SecurityEvent.event_type == 'login_failed')
            .where(SecurityEvent.user_id == user_id)
            .where(SecurityEvent.ip_address == ip_address)
            .where(SecurityEvent.timestamp > cutoff_time)
        ).all()
        
        if len(failed_attempts) >= 5:  # 5æ¬¡å¤±è´¥ç™»å½•
            self.log_security_event(
                session, 'suspicious_login_attempts', 'high',
                user_id=user_id, ip_address=ip_address,
                details={'failed_attempts': len(failed_attempts)}
            )
            return True
        
        return False
    
    def get_security_summary(self, session: Session, days: int = 7) -> Dict[str, Any]:
        """è·å–å®‰å…¨æ‘˜è¦"""
        cutoff_time = datetime.utcnow() - timedelta(days=days)
        
        # ç»Ÿè®¡å®‰å…¨äº‹ä»¶
        events = session.exec(
            select(SecurityEvent)
            .where(SecurityEvent.timestamp > cutoff_time)
        ).all()
        
        event_summary = {}
        severity_summary = {}
        
        for event in events:
            event_summary[event.event_type] = event_summary.get(event.event_type, 0) + 1
            severity_summary[event.severity] = severity_summary.get(event.severity, 0) + 1
        
        return {
            "æ—¶é—´èŒƒå›´": f"æœ€è¿‘ {days} å¤©",
            "æ€»äº‹ä»¶æ•°": len(events),
            "äº‹ä»¶ç±»å‹ç»Ÿè®¡": event_summary,
            "ä¸¥é‡æ€§ç»Ÿè®¡": severity_summary,
            "æœªè§£å†³äº‹ä»¶": len([e for e in events if not e.resolved])
        }

# è®¿é—®æ§åˆ¶
class AccessControl:
    """è®¿é—®æ§åˆ¶ç®¡ç†"""
    
    def __init__(self, security_config: SecurityConfig):
        self.security_config = security_config
        self.rate_limits = {}  # IP -> (count, reset_time)
    
    def check_rate_limit(self, ip_address: str, limit: int = 100, 
                        window: int = 3600) -> bool:
        """æ£€æŸ¥é€Ÿç‡é™åˆ¶"""
        now = datetime.utcnow()
        
        if ip_address in self.rate_limits:
            count, reset_time = self.rate_limits[ip_address]
            
            if now > reset_time:
                # é‡ç½®è®¡æ•°å™¨
                self.rate_limits[ip_address] = (1, now + timedelta(seconds=window))
                return True
            elif count >= limit:
                return False
            else:
                self.rate_limits[ip_address] = (count + 1, reset_time)
                return True
        else:
            self.rate_limits[ip_address] = (1, now + timedelta(seconds=window))
            return True
    
    def validate_input(self, data: str, max_length: int = 1000) -> bool:
        """éªŒè¯è¾“å…¥æ•°æ®"""
        # é•¿åº¦æ£€æŸ¥
        if len(data) > max_length:
            return False
        
        # SQLæ³¨å…¥æ£€æŸ¥
        sql_patterns = [
            'union', 'select', 'insert', 'update', 'delete', 'drop',
            'exec', 'execute', 'sp_', 'xp_', '--', '/*', '*/', ';'
        ]
        
        data_lower = data.lower()
        for pattern in sql_patterns:
            if pattern in data_lower:
                logging.warning(f"å¯ç–‘è¾“å…¥æ£€æµ‹: {pattern} in {data[:50]}...")
                return False
        
        return True
    
    def sanitize_input(self, data: str) -> str:
        """æ¸…ç†è¾“å…¥æ•°æ®"""
        # ç§»é™¤å±é™©å­—ç¬¦
        dangerous_chars = ['<', '>', '"', "'", '&', ';']
        for char in dangerous_chars:
            data = data.replace(char, '')
        
        return data.strip()

# ä½¿ç”¨ç¤ºä¾‹
def setup_security():
    """è®¾ç½®å®‰å…¨é…ç½®"""
    # åˆ›å»ºå®‰å…¨é…ç½®
    security_config = SecurityConfig()
    
    # åˆ›å»ºæ•°æ®åº“å¼•æ“
    engine = create_engine("postgresql://user:pass@localhost/db")
    
    # è®¾ç½®æ•°æ®åº“å®‰å…¨
    db_security = DatabaseSecurity(engine, security_config)
    
    # è®¾ç½®è®¿é—®æ§åˆ¶
    access_control = AccessControl(security_config)
    
    return security_config, db_security, access_control
```

### 1.3 ç¯å¢ƒå˜é‡ç®¡ç†

```python
# environment_manager.py
from typing import Dict, Any, Optional, List
import os
import json
from pathlib import Path
import logging

class EnvironmentManager:
    """ç¯å¢ƒå˜é‡ç®¡ç†å™¨"""
    
    def __init__(self, env_file: Optional[str] = None):
        self.env_file = env_file or '.env'
        self.required_vars = set()
        self.default_values = {}
        self.validators = {}
        self._load_env_file()
    
    def _load_env_file(self):
        """åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶"""
        env_path = Path(self.env_file)
        if env_path.exists():
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip().strip('"\'')
                        if key not in os.environ:
                            os.environ[key] = value
    
    def require(self, var_name: str, default: Any = None, 
               validator: Optional[callable] = None) -> 'EnvironmentManager':
        """æ ‡è®°å¿…éœ€çš„ç¯å¢ƒå˜é‡"""
        self.required_vars.add(var_name)
        if default is not None:
            self.default_values[var_name] = default
        if validator:
            self.validators[var_name] = validator
        return self
    
    def get(self, var_name: str, default: Any = None, 
           var_type: type = str) -> Any:
        """è·å–ç¯å¢ƒå˜é‡"""
        value = os.getenv(var_name)
        
        if value is None:
            if var_name in self.default_values:
                value = self.default_values[var_name]
            elif default is not None:
                value = default
            elif var_name in self.required_vars:
                raise ValueError(f"å¿…éœ€çš„ç¯å¢ƒå˜é‡ {var_name} æœªè®¾ç½®")
            else:
                return None
        
        # ç±»å‹è½¬æ¢
        if var_type == bool:
            return str(value).lower() in ('true', '1', 'yes', 'on')
        elif var_type == int:
            return int(value)
        elif var_type == float:
            return float(value)
        elif var_type == list:
            return [item.strip() for item in str(value).split(',') if item.strip()]
        elif var_type == dict:
            return json.loads(value)
        else:
            return str(value)
    
    def validate(self) -> List[str]:
        """éªŒè¯ç¯å¢ƒå˜é‡"""
        errors = []
        
        # æ£€æŸ¥å¿…éœ€å˜é‡
        for var_name in self.required_vars:
            value = os.getenv(var_name)
            if value is None and var_name not in self.default_values:
                errors.append(f"ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {var_name}")
            
            # è¿è¡ŒéªŒè¯å™¨
            if var_name in self.validators and value is not None:
                validator = self.validators[var_name]
                try:
                    if not validator(value):
                        errors.append(f"ç¯å¢ƒå˜é‡ {var_name} éªŒè¯å¤±è´¥")
                except Exception as e:
                    errors.append(f"ç¯å¢ƒå˜é‡ {var_name} éªŒè¯é”™è¯¯: {e}")
        
        return errors
    
    def get_config_dict(self, prefix: str = '') -> Dict[str, Any]:
        """è·å–é…ç½®å­—å…¸"""
        config = {}
        for key, value in os.environ.items():
            if key.startswith(prefix):
                config_key = key[len(prefix):] if prefix else key
                config[config_key] = value
        return config
    
    def export_template(self, file_path: str):
        """å¯¼å‡ºç¯å¢ƒå˜é‡æ¨¡æ¿"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("# ç¯å¢ƒå˜é‡é…ç½®æ¨¡æ¿\n\n")
            
            for var_name in sorted(self.required_vars):
                default_value = self.default_values.get(var_name, '')
                f.write(f"# {var_name}\n")
                f.write(f"{var_name}={default_value}\n\n")

# ç”Ÿäº§ç¯å¢ƒé…ç½®éªŒè¯å™¨
def validate_database_url(url: str) -> bool:
    """éªŒè¯æ•°æ®åº“URL"""
    return url.startswith(('postgresql://', 'mysql://', 'sqlite:///'))

def validate_port(port: str) -> bool:
    """éªŒè¯ç«¯å£å·"""
    try:
        port_num = int(port)
        return 1 <= port_num <= 65535
    except ValueError:
        return False

def validate_log_level(level: str) -> bool:
    """éªŒè¯æ—¥å¿—çº§åˆ«"""
    return level.upper() in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']

# é…ç½®ç”Ÿäº§ç¯å¢ƒ
def setup_production_environment():
    """è®¾ç½®ç”Ÿäº§ç¯å¢ƒé…ç½®"""
    env_manager = EnvironmentManager()
    
    # é…ç½®å¿…éœ€çš„ç¯å¢ƒå˜é‡
    env_manager.require('DATABASE_URL', validator=validate_database_url)
    env_manager.require('SECRET_KEY')
    env_manager.require('JWT_SECRET')
    env_manager.require('ENCRYPTION_KEY')
    env_manager.require('ENVIRONMENT', default='production')
    env_manager.require('LOG_LEVEL', default='INFO', validator=validate_log_level)
    env_manager.require('PORT', default='8000', validator=validate_port)
    env_manager.require('DB_POOL_SIZE', default='20')
    env_manager.require('DB_MAX_OVERFLOW', default='30')
    env_manager.require('REDIS_URL', default='redis://localhost:6379')
    
    # éªŒè¯é…ç½®
    errors = env_manager.validate()
    if errors:
        for error in errors:
            logging.error(error)
        raise ValueError(f"ç¯å¢ƒé…ç½®éªŒè¯å¤±è´¥: {errors}")
    
    # è¿”å›é…ç½®
    return {
        'database_url': env_manager.get('DATABASE_URL'),
        'secret_key': env_manager.get('SECRET_KEY'),
        'jwt_secret': env_manager.get('JWT_SECRET'),
        'encryption_key': env_manager.get('ENCRYPTION_KEY'),
        'environment': env_manager.get('ENVIRONMENT'),
        'log_level': env_manager.get('LOG_LEVEL'),
        'port': env_manager.get('PORT', var_type=int),
        'db_pool_size': env_manager.get('DB_POOL_SIZE', var_type=int),
        'db_max_overflow': env_manager.get('DB_MAX_OVERFLOW', var_type=int),
        'redis_url': env_manager.get('REDIS_URL'),
        'debug': env_manager.get('DEBUG', default=False, var_type=bool),
        'allowed_hosts': env_manager.get('ALLOWED_HOSTS', default='*', var_type=list)
    }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    try:
        config = setup_production_environment()
        print("ç”Ÿäº§ç¯å¢ƒé…ç½®åŠ è½½æˆåŠŸ:")
        for key, value in config.items():
            if 'secret' in key.lower() or 'key' in key.lower():
                print(f"{key}: {'*' * len(str(value))}")
            else:
                print(f"{key}: {value}")
    except Exception as e:
        logging.error(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
        exit(1)
```

---

## 2. ç›‘æ§ä¸å‘Šè­¦

### 2.1 åº”ç”¨ç›‘æ§

```python
# application_monitoring.py
from sqlmodel import SQLModel, Field, Session, select
from sqlalchemy import create_engine, event, text
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime, timedelta
import time
import psutil
import threading
import logging
import json
from collections import defaultdict, deque
from dataclasses import dataclass

@dataclass
class MetricPoint:
    """æŒ‡æ ‡æ•°æ®ç‚¹"""
    timestamp: datetime
    value: float
    tags: Dict[str, str] = None

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, max_points: int = 1000):
        self.metrics = defaultdict(lambda: deque(maxlen=max_points))
        self.counters = defaultdict(int)
        self.gauges = defaultdict(float)
        self.histograms = defaultdict(list)
        self._lock = threading.Lock()
    
    def counter(self, name: str, value: int = 1, tags: Dict[str, str] = None):
        """è®¡æ•°å™¨æŒ‡æ ‡"""
        with self._lock:
            key = self._make_key(name, tags)
            self.counters[key] += value
            self.metrics[key].append(MetricPoint(datetime.utcnow(), self.counters[key], tags))
    
    def gauge(self, name: str, value: float, tags: Dict[str, str] = None):
        """ä»ªè¡¨ç›˜æŒ‡æ ‡"""
        with self._lock:
            key = self._make_key(name, tags)
            self.gauges[key] = value
            self.metrics[key].append(MetricPoint(datetime.utcnow(), value, tags))
    
    def histogram(self, name: str, value: float, tags: Dict[str, str] = None):
        """ç›´æ–¹å›¾æŒ‡æ ‡"""
        with self._lock:
            key = self._make_key(name, tags)
            self.histograms[key].append(value)
            self.metrics[key].append(MetricPoint(datetime.utcnow(), value, tags))
    
    def timer(self, name: str, tags: Dict[str, str] = None):
        """è®¡æ—¶å™¨è£…é¥°å™¨"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    self.histogram(f"{name}.success", time.time() - start_time, tags)
                    return result
                except Exception as e:
                    self.histogram(f"{name}.error", time.time() - start_time, tags)
                    self.counter(f"{name}.error_count", 1, tags)
                    raise
            return wrapper
        return decorator
    
    def _make_key(self, name: str, tags: Dict[str, str] = None) -> str:
        """ç”ŸæˆæŒ‡æ ‡é”®"""
        if tags:
            tag_str = ','.join(f"{k}={v}" for k, v in sorted(tags.items()))
            return f"{name}[{tag_str}]"
        return name
    
    def get_metrics(self, name_pattern: str = None, 
                   since: datetime = None) -> Dict[str, List[MetricPoint]]:
        """è·å–æŒ‡æ ‡æ•°æ®"""
        with self._lock:
            result = {}
            for key, points in self.metrics.items():
                if name_pattern and name_pattern not in key:
                    continue
                
                filtered_points = points
                if since:
                    filtered_points = [p for p in points if p.timestamp >= since]
                
                result[key] = list(filtered_points)
            
            return result
    
    def get_summary(self, name: str, tags: Dict[str, str] = None, 
                   window_minutes: int = 60) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        key = self._make_key(name, tags)
        since = datetime.utcnow() - timedelta(minutes=window_minutes)
        
        points = [p for p in self.metrics[key] if p.timestamp >= since]
        if not points:
            return {"message": "æ²¡æœ‰æ•°æ®"}
        
        values = [p.value for p in points]
        return {
            "count": len(values),
            "min": min(values),
            "max": max(values),
            "avg": sum(values) / len(values),
            "latest": values[-1] if values else None,
            "window_minutes": window_minutes
        }

class ApplicationMonitor:
    """åº”ç”¨ç›‘æ§å™¨"""
    
    def __init__(self, engine, metrics_collector: MetricsCollector):
        self.engine = engine
        self.metrics = metrics_collector
        self.start_time = datetime.utcnow()
        self._setup_database_monitoring()
        self._setup_system_monitoring()
    
    def _setup_database_monitoring(self):
        """è®¾ç½®æ•°æ®åº“ç›‘æ§"""
        @event.listens_for(self.engine, "before_cursor_execute")
        def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            context._query_start_time = time.time()
            self.metrics.counter("database.queries.total")
        
        @event.listens_for(self.engine, "after_cursor_execute")
        def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            if hasattr(context, '_query_start_time'):
                execution_time = time.time() - context._query_start_time
                self.metrics.histogram("database.query.duration", execution_time)
                
                # æ…¢æŸ¥è¯¢ç›‘æ§
                if execution_time > 1.0:
                    self.metrics.counter("database.queries.slow")
                    logging.warning(f"æ…¢æŸ¥è¯¢: {execution_time:.3f}s - {statement[:100]}...")
        
        @event.listens_for(self.engine, "checkout")
        def on_checkout(dbapi_connection, connection_record, connection_proxy):
            self.metrics.counter("database.connections.checkout")
        
        @event.listens_for(self.engine, "checkin")
        def on_checkin(dbapi_connection, connection_record):
            self.metrics.counter("database.connections.checkin")
    
    def _setup_system_monitoring(self):
        """è®¾ç½®ç³»ç»Ÿç›‘æ§"""
        def collect_system_metrics():
            while True:
                try:
                    # CPUä½¿ç”¨ç‡
                    cpu_percent = psutil.cpu_percent(interval=1)
                    self.metrics.gauge("system.cpu.usage", cpu_percent)
                    
                    # å†…å­˜ä½¿ç”¨
                    memory = psutil.virtual_memory()
                    self.metrics.gauge("system.memory.usage", memory.percent)
                    self.metrics.gauge("system.memory.available", memory.available)
                    
                    # ç£ç›˜ä½¿ç”¨
                    disk = psutil.disk_usage('/')
                    self.metrics.gauge("system.disk.usage", disk.percent)
                    self.metrics.gauge("system.disk.free", disk.free)
                    
                    # ç½‘ç»œIO
                    net_io = psutil.net_io_counters()
                    self.metrics.gauge("system.network.bytes_sent", net_io.bytes_sent)
                    self.metrics.gauge("system.network.bytes_recv", net_io.bytes_recv)
                    
                    # æ•°æ®åº“è¿æ¥æ± çŠ¶æ€
                    if hasattr(self.engine, 'pool'):
                        pool = self.engine.pool
                        self.metrics.gauge("database.pool.size", pool.size())
                        self.metrics.gauge("database.pool.checked_in", pool.checkedin())
                        self.metrics.gauge("database.pool.checked_out", pool.checkedout())
                        self.metrics.gauge("database.pool.overflow", pool.overflow())
                    
                except Exception as e:
                    logging.error(f"ç³»ç»ŸæŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
                
                time.sleep(60)  # æ¯åˆ†é’Ÿæ”¶é›†ä¸€æ¬¡
        
        # å¯åŠ¨åå°çº¿ç¨‹
        thread = threading.Thread(target=collect_system_metrics, daemon=True)
        thread.start()
    
    def get_health_status(self) -> Dict[str, Any]:
        """è·å–å¥åº·çŠ¶æ€"""
        try:
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            db_status = "healthy"
        except Exception as e:
            db_status = f"unhealthy: {e}"
        
        # è·å–ç³»ç»ŸæŒ‡æ ‡
        cpu_usage = self.metrics.get_summary("system.cpu.usage", window_minutes=5)
        memory_usage = self.metrics.get_summary("system.memory.usage", window_minutes=5)
        
        # è®¡ç®—è¿è¡Œæ—¶é—´
        uptime = datetime.utcnow() - self.start_time
        
        return {
            "status": "healthy" if db_status == "healthy" else "unhealthy",
            "timestamp": datetime.utcnow().isoformat(),
            "uptime_seconds": uptime.total_seconds(),
            "database": db_status,
            "cpu_usage": cpu_usage.get("latest", 0),
            "memory_usage": memory_usage.get("latest", 0),
            "version": "1.0.0",  # åº”ç”¨ç‰ˆæœ¬
            "environment": os.getenv("ENVIRONMENT", "unknown")
        }
    
    def get_performance_report(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        since = datetime.utcnow() - timedelta(hours=hours)
        
        # æ•°æ®åº“æ€§èƒ½
        query_duration = self.metrics.get_summary("database.query.duration", window_minutes=hours*60)
        total_queries = self.metrics.get_summary("database.queries.total", window_minutes=hours*60)
        slow_queries = self.metrics.get_summary("database.queries.slow", window_minutes=hours*60)
        
        # ç³»ç»Ÿæ€§èƒ½
        cpu_usage = self.metrics.get_summary("system.cpu.usage", window_minutes=hours*60)
        memory_usage = self.metrics.get_summary("system.memory.usage", window_minutes=hours*60)
        
        return {
            "æ—¶é—´èŒƒå›´": f"æœ€è¿‘ {hours} å°æ—¶",
            "æ•°æ®åº“æ€§èƒ½": {
                "æŸ¥è¯¢æ€»æ•°": total_queries.get("latest", 0),
                "æ…¢æŸ¥è¯¢æ•°": slow_queries.get("latest", 0),
                "å¹³å‡æŸ¥è¯¢æ—¶é—´": f"{query_duration.get('avg', 0):.3f}s",
                "æœ€å¤§æŸ¥è¯¢æ—¶é—´": f"{query_duration.get('max', 0):.3f}s"
            },
            "ç³»ç»Ÿæ€§èƒ½": {
                "å¹³å‡CPUä½¿ç”¨ç‡": f"{cpu_usage.get('avg', 0):.1f}%",
                "æœ€å¤§CPUä½¿ç”¨ç‡": f"{cpu_usage.get('max', 0):.1f}%",
                "å¹³å‡å†…å­˜ä½¿ç”¨ç‡": f"{memory_usage.get('avg', 0):.1f}%",
                "æœ€å¤§å†…å­˜ä½¿ç”¨ç‡": f"{memory_usage.get('max', 0):.1f}%"
            }
        }

# å‘Šè­¦ç³»ç»Ÿ
class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self, metrics_collector: MetricsCollector):
        self.metrics = metrics_collector
        self.alert_rules = []
        self.alert_handlers = []
        self.alert_history = deque(maxlen=1000)
        self._setup_default_rules()
    
    def add_rule(self, name: str, metric_name: str, condition: str, 
                threshold: float, severity: str = "warning"):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        rule = {
            "name": name,
            "metric_name": metric_name,
            "condition": condition,  # ">", "<", ">=", "<=", "=="
            "threshold": threshold,
            "severity": severity,
            "last_triggered": None
        }
        self.alert_rules.append(rule)
    
    def add_handler(self, handler: Callable[[Dict[str, Any]], None]):
        """æ·»åŠ å‘Šè­¦å¤„ç†å™¨"""
        self.alert_handlers.append(handler)
    
    def _setup_default_rules(self):
        """è®¾ç½®é»˜è®¤å‘Šè­¦è§„åˆ™"""
        self.add_rule("é«˜CPUä½¿ç”¨ç‡", "system.cpu.usage", ">", 80, "warning")
        self.add_rule("æé«˜CPUä½¿ç”¨ç‡", "system.cpu.usage", ">", 95, "critical")
        self.add_rule("é«˜å†…å­˜ä½¿ç”¨ç‡", "system.memory.usage", ">", 85, "warning")
        self.add_rule("æé«˜å†…å­˜ä½¿ç”¨ç‡", "system.memory.usage", ">", 95, "critical")
        self.add_rule("æ…¢æŸ¥è¯¢è¿‡å¤š", "database.queries.slow", ">", 10, "warning")
        self.add_rule("ç£ç›˜ç©ºé—´ä¸è¶³", "system.disk.usage", ">", 90, "critical")
    
    def check_alerts(self):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        for rule in self.alert_rules:
            try:
                summary = self.metrics.get_summary(rule["metric_name"], window_minutes=5)
                if summary.get("latest") is None:
                    continue
                
                current_value = summary["latest"]
                threshold = rule["threshold"]
                condition = rule["condition"]
                
                triggered = False
                if condition == ">" and current_value > threshold:
                    triggered = True
                elif condition == "<" and current_value < threshold:
                    triggered = True
                elif condition == ">=" and current_value >= threshold:
                    triggered = True
                elif condition == "<=" and current_value <= threshold:
                    triggered = True
                elif condition == "==" and current_value == threshold:
                    triggered = True
                
                if triggered:
                    # é¿å…é‡å¤å‘Šè­¦ï¼ˆ5åˆ†é’Ÿå†…ï¼‰
                    now = datetime.utcnow()
                    if (rule["last_triggered"] is None or 
                        (now - rule["last_triggered"]).total_seconds() > 300):
                        
                        alert = {
                            "name": rule["name"],
                            "metric_name": rule["metric_name"],
                            "current_value": current_value,
                            "threshold": threshold,
                            "condition": condition,
                            "severity": rule["severity"],
                            "timestamp": now,
                            "message": f"{rule['name']}: {rule['metric_name']} {condition} {threshold} (å½“å‰å€¼: {current_value})"
                        }
                        
                        self._trigger_alert(alert)
                        rule["last_triggered"] = now
            
            except Exception as e:
                logging.error(f"å‘Šè­¦æ£€æŸ¥é”™è¯¯: {e}")
    
    def _trigger_alert(self, alert: Dict[str, Any]):
        """è§¦å‘å‘Šè­¦"""
        self.alert_history.append(alert)
        
        # è®°å½•æ—¥å¿—
        log_level = logging.CRITICAL if alert["severity"] == "critical" else logging.WARNING
        logging.log(log_level, alert["message"])
        
        # è°ƒç”¨å‘Šè­¦å¤„ç†å™¨
        for handler in self.alert_handlers:
            try:
                handler(alert)
            except Exception as e:
                logging.error(f"å‘Šè­¦å¤„ç†å™¨é”™è¯¯: {e}")
    
    def get_active_alerts(self, minutes: int = 60) -> List[Dict[str, Any]]:
        """è·å–æ´»è·ƒå‘Šè­¦"""
        cutoff_time = datetime.utcnow() - timedelta(minutes=minutes)
        return [alert for alert in self.alert_history if alert["timestamp"] > cutoff_time]
    
    def start_monitoring(self, interval: int = 60):
        """å¯åŠ¨ç›‘æ§"""
        def monitor_loop():
            while True:
                try:
                    self.check_alerts()
                except Exception as e:
                    logging.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(interval)
        
        thread = threading.Thread(target=monitor_loop, daemon=True)
        thread.start()
        logging.info(f"å‘Šè­¦ç›‘æ§å·²å¯åŠ¨ï¼Œæ£€æŸ¥é—´éš”: {interval}ç§’")

# å‘Šè­¦å¤„ç†å™¨ç¤ºä¾‹
def email_alert_handler(alert: Dict[str, Any]):
    """é‚®ä»¶å‘Šè­¦å¤„ç†å™¨"""
    # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶å‘é€åŠŸèƒ½
    print(f"ğŸ“§ é‚®ä»¶å‘Šè­¦: {alert['message']}")

def slack_alert_handler(alert: Dict[str, Any]):
    """Slackå‘Šè­¦å¤„ç†å™¨"""
    # è¿™é‡Œå¯ä»¥é›†æˆSlack API
    print(f"ğŸ’¬ Slackå‘Šè­¦: {alert['message']}")

def webhook_alert_handler(alert: Dict[str, Any]):
    """Webhookå‘Šè­¦å¤„ç†å™¨"""
    # è¿™é‡Œå¯ä»¥å‘é€HTTPè¯·æ±‚åˆ°å‘Šè­¦ç³»ç»Ÿ
    print(f"ğŸ”— Webhookå‘Šè­¦: {alert['message']}")

# ä½¿ç”¨ç¤ºä¾‹
def setup_monitoring():
    """è®¾ç½®ç›‘æ§ç³»ç»Ÿ"""
    # åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨
    metrics = MetricsCollector()
    
    # åˆ›å»ºæ•°æ®åº“å¼•æ“
    engine = create_engine("postgresql://user:pass@localhost/db")
    
    # åˆ›å»ºåº”ç”¨ç›‘æ§å™¨
    app_monitor = ApplicationMonitor(engine, metrics)
    
    # åˆ›å»ºå‘Šè­¦ç®¡ç†å™¨
    alert_manager = AlertManager(metrics)
    
    # æ·»åŠ å‘Šè­¦å¤„ç†å™¨
    alert_manager.add_handler(email_alert_handler)
    alert_manager.add_handler(slack_alert_handler)
    
    # å¯åŠ¨å‘Šè­¦ç›‘æ§
    alert_manager.start_monitoring()
    
    return app_monitor, alert_manager, metrics
```

### 2.2 æ—¥å¿—ç®¡ç†

```python
# logging_config.py
import logging
import logging.handlers
from typing import Dict, Any, Optional
import json
import os
from datetime import datetime
from pathlib import Path
import gzip
import shutil

class StructuredFormatter(logging.Formatter):
    """ç»“æ„åŒ–æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    def format(self, record: logging.LogRecord) -> str:
        # åŸºç¡€æ—¥å¿—ä¿¡æ¯
        log_entry = {
            "timestamp": datetime.utcfromtimestamp(record.created).isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        
        # æ·»åŠ å¼‚å¸¸ä¿¡æ¯
        if record.exc_info:
            log_entry["exception"] = self.formatException(record.exc_info)
        
        # æ·»åŠ è‡ªå®šä¹‰å­—æ®µ
        if hasattr(record, 'user_id'):
            log_entry["user_id"] = record.user_id
        if hasattr(record, 'request_id'):
            log_entry["request_id"] = record.request_id
        if hasattr(record, 'ip_address'):
            log_entry["ip_address"] = record.ip_address
        if hasattr(record, 'extra_data'):
            log_entry["extra_data"] = record.extra_data
        
        return json.dumps(log_entry, ensure_ascii=False)

class DatabaseLogHandler(logging.Handler):
    """æ•°æ®åº“æ—¥å¿—å¤„ç†å™¨"""
    
    def __init__(self, engine, table_name: str = "application_logs"):
        super().__init__()
        self.engine = engine
        self.table_name = table_name
        self._ensure_table_exists()
    
    def _ensure_table_exists(self):
        """ç¡®ä¿æ—¥å¿—è¡¨å­˜åœ¨"""
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.table_name} (
            id SERIAL PRIMARY KEY,
            timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
            level VARCHAR(20) NOT NULL,
            logger VARCHAR(100) NOT NULL,
            message TEXT NOT NULL,
            module VARCHAR(100),
            function VARCHAR(100),
            line_number INTEGER,
            user_id INTEGER,
            request_id VARCHAR(100),
            ip_address INET,
            exception TEXT,
            extra_data JSONB,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
        );
        
        CREATE INDEX IF NOT EXISTS idx_{self.table_name}_timestamp ON {self.table_name}(timestamp);
        CREATE INDEX IF NOT EXISTS idx_{self.table_name}_level ON {self.table_name}(level);
        CREATE INDEX IF NOT EXISTS idx_{self.table_name}_user_id ON {self.table_name}(user_id);
        CREATE INDEX IF NOT EXISTS idx_{self.table_name}_request_id ON {self.table_name}(request_id);
        """
        
        try:
            with self.engine.connect() as conn:
                conn.execute(create_table_sql)
                conn.commit()
        except Exception as e:
            print(f"åˆ›å»ºæ—¥å¿—è¡¨å¤±è´¥: {e}")
    
    def emit(self, record: logging.LogRecord):
        """å‘é€æ—¥å¿—è®°å½•åˆ°æ•°æ®åº“"""
        try:
            # å‡†å¤‡æ•°æ®
            data = {
                "timestamp": datetime.utcfromtimestamp(record.created),
                "level": record.levelname,
                "logger": record.name,
                "message": record.getMessage(),
                "module": record.module,
                "function": record.funcName,
                "line_number": record.lineno,
                "user_id": getattr(record, 'user_id', None),
                "request_id": getattr(record, 'request_id', None),
                "ip_address": getattr(record, 'ip_address', None),
                "exception": self.formatException(record.exc_info) if record.exc_info else None,
                "extra_data": getattr(record, 'extra_data', None)
            }
            
            # æ’å…¥æ•°æ®åº“
            insert_sql = f"""
            INSERT INTO {self.table_name} 
            (timestamp, level, logger, message, module, function, line_number, 
             user_id, request_id, ip_address, exception, extra_data)
            VALUES (%(timestamp)s, %(level)s, %(logger)s, %(message)s, %(module)s, 
                   %(function)s, %(line_number)s, %(user_id)s, %(request_id)s, 
                   %(ip_address)s, %(exception)s, %(extra_data)s)
            """
            
            with self.engine.connect() as conn:
                conn.execute(insert_sql, data)
                conn.commit()
        
        except Exception as e:
            # é¿å…æ—¥å¿—å¾ªç¯
            self.handleError(record)

class CompressedRotatingFileHandler(logging.handlers.RotatingFileHandler):
    """å‹ç¼©è½®è½¬æ–‡ä»¶å¤„ç†å™¨"""
    
    def doRollover(self):
        """æ‰§è¡Œæ—¥å¿—è½®è½¬å¹¶å‹ç¼©æ—§æ–‡ä»¶"""
        if self.stream:
            self.stream.close()
            self.stream = None
        
        if self.backupCount > 0:
            for i in range(self.backupCount - 1, 0, -1):
                sfn = self.rotation_filename("%s.%d.gz" % (self.baseFilename, i))
                dfn = self.rotation_filename("%s.%d.gz" % (self.baseFilename, i + 1))
                if os.path.exists(sfn):
                    if os.path.exists(dfn):
                        os.remove(dfn)
                    os.rename(sfn, dfn)
            
            # å‹ç¼©å½“å‰æ–‡ä»¶
            dfn = self.rotation_filename(self.baseFilename + ".1.gz")
            if os.path.exists(dfn):
                os.remove(dfn)
            
            with open(self.baseFilename, 'rb') as f_in:
                with gzip.open(dfn, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            os.remove(self.baseFilename)
        
        if not self.delay:
            self.stream = self._open()

class LoggingConfig:
    """æ—¥å¿—é…ç½®ç®¡ç†"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.loggers = {}
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path(self.config.get('log_dir', 'logs'))
        log_dir.mkdir(exist_ok=True)
        
        # æ ¹æ—¥å¿—å™¨é…ç½®
        root_logger = logging.getLogger()
        root_logger.setLevel(getattr(logging, self.config.get('level', 'INFO')))
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        root_logger.handlers.clear()
        
        # æ§åˆ¶å°å¤„ç†å™¨
        if self.config.get('console', {}).get('enabled', True):
            console_handler = logging.StreamHandler()
            console_handler.setLevel(getattr(logging, self.config.get('console', {}).get('level', 'INFO')))
            
            if self.config.get('console', {}).get('structured', False):
                console_handler.setFormatter(StructuredFormatter())
            else:
                console_handler.setFormatter(logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
                ))
            
            root_logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        if self.config.get('file', {}).get('enabled', True):
            file_config = self.config.get('file', {})
            log_file = log_dir / file_config.get('filename', 'application.log')
            
            if file_config.get('rotate', True):
                file_handler = CompressedRotatingFileHandler(
                    log_file,
                    maxBytes=file_config.get('max_bytes', 10*1024*1024),  # 10MB
                    backupCount=file_config.get('backup_count', 5)
                )
            else:
                file_handler = logging.FileHandler(log_file)
            
            file_handler.setLevel(getattr(logging, file_config.get('level', 'INFO')))
            
            if file_config.get('structured', True):
                file_handler.setFormatter(StructuredFormatter())
            else:
                file_handler.setFormatter(logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
                ))
            
            root_logger.addHandler(file_handler)
        
        # æ•°æ®åº“å¤„ç†å™¨
        if self.config.get('database', {}).get('enabled', False):
            from sqlalchemy import create_engine
            engine = create_engine(self.config['database']['url'])
            
            db_handler = DatabaseLogHandler(engine, self.config['database'].get('table', 'application_logs'))
            db_handler.setLevel(getattr(logging, self.config.get('database', {}).get('level', 'WARNING')))
            
            root_logger.addHandler(db_handler)
        
        # é”™è¯¯æ–‡ä»¶å¤„ç†å™¨
        if self.config.get('error_file', {}).get('enabled', True):
            error_config = self.config.get('error_file', {})
            error_file = log_dir / error_config.get('filename', 'error.log')
            
            error_handler = CompressedRotatingFileHandler(
                error_file,
                maxBytes=error_config.get('max_bytes', 5*1024*1024),  # 5MB
                backupCount=error_config.get('backup_count', 10)
            )
            error_handler.setLevel(logging.ERROR)
            error_handler.setFormatter(StructuredFormatter())
            
            root_logger.addHandler(error_handler)
    
    def get_logger(self, name: str) -> logging.Logger:
        """è·å–æŒ‡å®šåç§°çš„æ—¥å¿—å™¨"""
        if name not in self.loggers:
            logger = logging.getLogger(name)
            self.loggers[name] = logger
        return self.loggers[name]
    
    def add_context(self, **kwargs) -> 'LogContext':
        """æ·»åŠ æ—¥å¿—ä¸Šä¸‹æ–‡"""
        return LogContext(**kwargs)

class LogContext:
    """æ—¥å¿—ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, **kwargs):
        self.context = kwargs
        self.old_factory = None
    
    def __enter__(self):
        self.old_factory = logging.getLogRecordFactory()
        
        def record_factory(*args, **kwargs):
            record = self.old_factory(*args, **kwargs)
            for key, value in self.context.items():
                setattr(record, key, value)
            return record
        
        logging.setLogRecordFactory(record_factory)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        logging.setLogRecordFactory(self.old_factory)

# æ—¥å¿—åˆ†æå·¥å…·
class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, engine):
        self.engine = engine
    
    def get_error_summary(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–é”™è¯¯æ‘˜è¦"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        query = """
        SELECT 
            level,
            COUNT(*) as count,
            COUNT(DISTINCT user_id) as affected_users,
            array_agg(DISTINCT logger) as loggers
        FROM application_logs 
        WHERE timestamp > %s AND level IN ('ERROR', 'CRITICAL')
        GROUP BY level
        ORDER BY count DESC
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query, (cutoff_time,))
            errors = result.fetchall()
        
        return {
            "æ—¶é—´èŒƒå›´": f"æœ€è¿‘ {hours} å°æ—¶",
            "é”™è¯¯ç»Ÿè®¡": [dict(row) for row in errors]
        }
    
    def get_top_errors(self, limit: int = 10, hours: int = 24) -> List[Dict[str, Any]]:
        """è·å–æœ€é¢‘ç¹çš„é”™è¯¯"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        query = """
        SELECT 
            message,
            COUNT(*) as count,
            MAX(timestamp) as last_occurrence,
            array_agg(DISTINCT user_id) FILTER (WHERE user_id IS NOT NULL) as affected_users
        FROM application_logs 
        WHERE timestamp > %s AND level IN ('ERROR', 'CRITICAL')
        GROUP BY message
        ORDER BY count DESC
        LIMIT %s
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query, (cutoff_time, limit))
            errors = result.fetchall()
        
        return [dict(row) for row in errors]
    
    def get_user_activity(self, user_id: int, hours: int = 24) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·æ´»åŠ¨æ—¥å¿—"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        query = """
        SELECT timestamp, level, logger, message, ip_address
        FROM application_logs 
        WHERE user_id = %s AND timestamp > %s
        ORDER BY timestamp DESC
        LIMIT 100
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query, (user_id, cutoff_time))
            logs = result.fetchall()
        
        return [dict(row) for row in logs]
    
    def cleanup_old_logs(self, days: int = 30) -> int:
        """æ¸…ç†æ—§æ—¥å¿—"""
        cutoff_time = datetime.utcnow() - timedelta(days=days)
        
        query = "DELETE FROM application_logs WHERE timestamp < %s"
        
        with self.engine.connect() as conn:
            result = conn.execute(query, (cutoff_time,))
            conn.commit()
            return result.rowcount

# ç”Ÿäº§ç¯å¢ƒæ—¥å¿—é…ç½®
def get_production_logging_config() -> Dict[str, Any]:
    """è·å–ç”Ÿäº§ç¯å¢ƒæ—¥å¿—é…ç½®"""
    return {
        "level": os.getenv("LOG_LEVEL", "INFO"),
        "log_dir": os.getenv("LOG_DIR", "/var/log/app"),
        "console": {
            "enabled": os.getenv("LOG_CONSOLE_ENABLED", "true").lower() == "true",
            "level": os.getenv("LOG_CONSOLE_LEVEL", "INFO"),
            "structured": os.getenv("LOG_CONSOLE_STRUCTURED", "false").lower() == "true"
        },
        "file": {
            "enabled": os.getenv("LOG_FILE_ENABLED", "true").lower() == "true",
            "filename": os.getenv("LOG_FILE_NAME", "application.log"),
            "level": os.getenv("LOG_FILE_LEVEL", "INFO"),
            "structured": os.getenv("LOG_FILE_STRUCTURED", "true").lower() == "true",
            "rotate": os.getenv("LOG_FILE_ROTATE", "true").lower() == "true",
            "max_bytes": int(os.getenv("LOG_FILE_MAX_BYTES", "10485760")),  # 10MB
            "backup_count": int(os.getenv("LOG_FILE_BACKUP_COUNT", "5"))
        },
        "error_file": {
            "enabled": os.getenv("LOG_ERROR_FILE_ENABLED", "true").lower() == "true",
            "filename": os.getenv("LOG_ERROR_FILE_NAME", "error.log"),
            "max_bytes": int(os.getenv("LOG_ERROR_FILE_MAX_BYTES", "5242880")),  # 5MB
            "backup_count": int(os.getenv("LOG_ERROR_FILE_BACKUP_COUNT", "10"))
        },
        "database": {
            "enabled": os.getenv("LOG_DB_ENABLED", "false").lower() == "true",
            "url": os.getenv("LOG_DB_URL", ""),
            "table": os.getenv("LOG_DB_TABLE", "application_logs"),
            "level": os.getenv("LOG_DB_LEVEL", "WARNING")
        }
    }

# ä½¿ç”¨ç¤ºä¾‹
def setup_production_logging():
    """è®¾ç½®ç”Ÿäº§ç¯å¢ƒæ—¥å¿—"""
    config = get_production_logging_config()
    logging_config = LoggingConfig(config)
    logging_config.setup_logging()
    
    # è·å–åº”ç”¨æ—¥å¿—å™¨
    app_logger = logging_config.get_logger("app")
    db_logger = logging_config.get_logger("database")
    security_logger = logging_config.get_logger("security")
    
    return app_logger, db_logger, security_logger

# æ—¥å¿—ä½¿ç”¨ç¤ºä¾‹
def example_logging_usage():
    """æ—¥å¿—ä½¿ç”¨ç¤ºä¾‹"""
    app_logger, db_logger, security_logger = setup_production_logging()
    
    # åŸºç¡€æ—¥å¿—
    app_logger.info("åº”ç”¨å¯åŠ¨")
    
    # å¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—
    with LogContext(user_id=123, request_id="req-456", ip_address="192.168.1.1"):
        app_logger.info("ç”¨æˆ·ç™»å½•æˆåŠŸ")
        db_logger.debug("æ‰§è¡ŒæŸ¥è¯¢: SELECT * FROM users")
        security_logger.warning("æ£€æµ‹åˆ°å¯ç–‘æ´»åŠ¨")
    
    # é”™è¯¯æ—¥å¿—
    try:
        raise ValueError("ç¤ºä¾‹é”™è¯¯")
    except Exception as e:
        app_logger.error("å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯", exc_info=True, extra={
             'extra_data': {'error_code': 'E001', 'user_action': 'login'}
         })
```

## 4. å¤‡ä»½ä¸æ¢å¤

### 4.1 æ•°æ®åº“å¤‡ä»½ç­–ç•¥

```python
import subprocess
import boto3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import tarfile
import gzip
import shutil

class DatabaseBackupManager:
    """æ•°æ®åº“å¤‡ä»½ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.backup_dir = Path(config.get('backup_dir', '/var/backups/db'))
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # S3 é…ç½®ï¼ˆå¯é€‰ï¼‰
        if config.get('s3_enabled', False):
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=config['s3_access_key'],
                aws_secret_access_key=config['s3_secret_key'],
                region_name=config.get('s3_region', 'us-east-1')
            )
            self.s3_bucket = config['s3_bucket']
        else:
            self.s3_client = None
    
    def create_backup(self, backup_type: str = 'full') -> Dict[str, Any]:
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"{self.config['database_name']}_{backup_type}_{timestamp}"
        backup_file = self.backup_dir / f"{backup_name}.sql"
        compressed_file = self.backup_dir / f"{backup_name}.sql.gz"
        
        try:
            # æ‰§è¡Œ pg_dump
            cmd = [
                'pg_dump',
                '-h', self.config['host'],
                '-p', str(self.config['port']),
                '-U', self.config['username'],
                '-d', self.config['database_name'],
                '--no-password',
                '--verbose',
                '--clean',
                '--if-exists',
                '--create',
                '-f', str(backup_file)
            ]
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['PGPASSWORD'] = self.config['password']
            
            # æ‰§è¡Œå¤‡ä»½
            result = subprocess.run(
                cmd,
                env=env,
                capture_output=True,
                text=True,
                timeout=3600  # 1å°æ—¶è¶…æ—¶
            )
            
            if result.returncode != 0:
                raise Exception(f"å¤‡ä»½å¤±è´¥: {result.stderr}")
            
            # å‹ç¼©å¤‡ä»½æ–‡ä»¶
            with open(backup_file, 'rb') as f_in:
                with gzip.open(compressed_file, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            # åˆ é™¤æœªå‹ç¼©æ–‡ä»¶
            backup_file.unlink()
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size = compressed_file.stat().st_size
            
            backup_info = {
                'name': backup_name,
                'file_path': str(compressed_file),
                'size_bytes': file_size,
                'size_mb': round(file_size / 1024 / 1024, 2),
                'timestamp': timestamp,
                'type': backup_type,
                'status': 'completed'
            }
            
            # ä¸Šä¼ åˆ° S3ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self.s3_client:
                s3_key = f"database-backups/{backup_name}.sql.gz"
                self.s3_client.upload_file(
                    str(compressed_file),
                    self.s3_bucket,
                    s3_key
                )
                backup_info['s3_key'] = s3_key
                backup_info['s3_bucket'] = self.s3_bucket
            
            return backup_info
            
        except Exception as e:
            return {
                'name': backup_name,
                'status': 'failed',
                'error': str(e),
                'timestamp': timestamp
            }
    
    def restore_backup(self, backup_file: str, target_db: Optional[str] = None) -> Dict[str, Any]:
        """æ¢å¤æ•°æ®åº“å¤‡ä»½"""
        if target_db is None:
            target_db = self.config['database_name']
        
        backup_path = Path(backup_file)
        if not backup_path.exists():
            # å°è¯•ä» S3 ä¸‹è½½
            if self.s3_client and backup_file.startswith('s3://'):
                s3_key = backup_file.replace(f's3://{self.s3_bucket}/', '')
                local_file = self.backup_dir / Path(s3_key).name
                self.s3_client.download_file(self.s3_bucket, s3_key, str(local_file))
                backup_path = local_file
            else:
                raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
        
        try:
            # è§£å‹æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if backup_path.suffix == '.gz':
                decompressed_file = backup_path.with_suffix('')
                with gzip.open(backup_path, 'rb') as f_in:
                    with open(decompressed_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                sql_file = decompressed_file
            else:
                sql_file = backup_path
            
            # æ‰§è¡Œæ¢å¤
            cmd = [
                'psql',
                '-h', self.config['host'],
                '-p', str(self.config['port']),
                '-U', self.config['username'],
                '-d', target_db,
                '-f', str(sql_file)
            ]
            
            env = os.environ.copy()
            env['PGPASSWORD'] = self.config['password']
            
            result = subprocess.run(
                cmd,
                env=env,
                capture_output=True,
                text=True,
                timeout=3600
            )
            
            if result.returncode != 0:
                raise Exception(f"æ¢å¤å¤±è´¥: {result.stderr}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if backup_path.suffix == '.gz' and sql_file.exists():
                sql_file.unlink()
            
            return {
                'status': 'success',
                'target_database': target_db,
                'backup_file': str(backup_path),
                'restored_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e),
                'target_database': target_db,
                'backup_file': str(backup_path)
            }
    
    def list_backups(self, include_s3: bool = True) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []
        
        # æœ¬åœ°å¤‡ä»½
        for backup_file in self.backup_dir.glob('*.sql.gz'):
            stat = backup_file.stat()
            backups.append({
                'name': backup_file.stem.replace('.sql', ''),
                'file_path': str(backup_file),
                'size_bytes': stat.st_size,
                'size_mb': round(stat.st_size / 1024 / 1024, 2),
                'created_at': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                'location': 'local'
            })
        
        # S3 å¤‡ä»½
        if include_s3 and self.s3_client:
            try:
                response = self.s3_client.list_objects_v2(
                    Bucket=self.s3_bucket,
                    Prefix='database-backups/'
                )
                
                for obj in response.get('Contents', []):
                    backups.append({
                        'name': Path(obj['Key']).stem.replace('.sql', ''),
                        's3_key': obj['Key'],
                        'size_bytes': obj['Size'],
                        'size_mb': round(obj['Size'] / 1024 / 1024, 2),
                        'created_at': obj['LastModified'].isoformat(),
                        'location': 's3'
                    })
            except Exception as e:
                print(f"è·å– S3 å¤‡ä»½åˆ—è¡¨å¤±è´¥: {e}")
        
        return sorted(backups, key=lambda x: x['created_at'], reverse=True)
    
    def cleanup_old_backups(self, keep_days: int = 30, keep_count: int = 10) -> Dict[str, Any]:
        """æ¸…ç†æ—§å¤‡ä»½"""
        cutoff_date = datetime.now() - timedelta(days=keep_days)
        deleted_local = 0
        deleted_s3 = 0
        
        # æ¸…ç†æœ¬åœ°å¤‡ä»½
        local_backups = []
        for backup_file in self.backup_dir.glob('*.sql.gz'):
            stat = backup_file.stat()
            created_at = datetime.fromtimestamp(stat.st_ctime)
            local_backups.append((backup_file, created_at))
        
        # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
        local_backups.sort(key=lambda x: x[1], reverse=True)
        
        for i, (backup_file, created_at) in enumerate(local_backups):
            if i >= keep_count or created_at < cutoff_date:
                backup_file.unlink()
                deleted_local += 1
        
        # æ¸…ç† S3 å¤‡ä»½
        if self.s3_client:
            try:
                response = self.s3_client.list_objects_v2(
                    Bucket=self.s3_bucket,
                    Prefix='database-backups/'
                )
                
                s3_backups = []
                for obj in response.get('Contents', []):
                    s3_backups.append((obj['Key'], obj['LastModified']))
                
                s3_backups.sort(key=lambda x: x[1], reverse=True)
                
                for i, (key, last_modified) in enumerate(s3_backups):
                    if i >= keep_count or last_modified.replace(tzinfo=None) < cutoff_date:
                        self.s3_client.delete_object(Bucket=self.s3_bucket, Key=key)
                        deleted_s3 += 1
                        
            except Exception as e:
                print(f"æ¸…ç† S3 å¤‡ä»½å¤±è´¥: {e}")
        
        return {
            'deleted_local': deleted_local,
            'deleted_s3': deleted_s3,
            'cutoff_date': cutoff_date.isoformat(),
            'keep_count': keep_count
        }

class BackupScheduler:
    """å¤‡ä»½è°ƒåº¦å™¨"""
    
    def __init__(self, backup_manager: DatabaseBackupManager):
        self.backup_manager = backup_manager
        self.schedules = []
    
    def add_schedule(self, schedule_type: str, frequency: str, backup_type: str = 'full'):
        """æ·»åŠ å¤‡ä»½è®¡åˆ’"""
        self.schedules.append({
            'type': schedule_type,
            'frequency': frequency,
            'backup_type': backup_type,
            'last_run': None
        })
    
    def should_run_backup(self, schedule: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿è¡Œå¤‡ä»½"""
        if schedule['last_run'] is None:
            return True
        
        last_run = datetime.fromisoformat(schedule['last_run'])
        now = datetime.now()
        
        if schedule['frequency'] == 'daily':
            return (now - last_run).days >= 1
        elif schedule['frequency'] == 'weekly':
            return (now - last_run).days >= 7
        elif schedule['frequency'] == 'monthly':
            return (now - last_run).days >= 30
        
        return False
    
    def run_scheduled_backups(self) -> List[Dict[str, Any]]:
        """è¿è¡Œè®¡åˆ’çš„å¤‡ä»½"""
        results = []
        
        for schedule in self.schedules:
            if self.should_run_backup(schedule):
                result = self.backup_manager.create_backup(schedule['backup_type'])
                result['schedule_type'] = schedule['type']
                result['frequency'] = schedule['frequency']
                
                if result['status'] == 'completed':
                    schedule['last_run'] = datetime.now().isoformat()
                
                results.append(result)
        
        return results

# å¤‡ä»½é…ç½®ç¤ºä¾‹
def get_backup_config() -> Dict[str, Any]:
    """è·å–å¤‡ä»½é…ç½®"""
    return {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', '5432')),
        'username': os.getenv('DB_USER', 'postgres'),
        'password': os.getenv('DB_PASSWORD', ''),
        'database_name': os.getenv('DB_NAME', 'myapp'),
        'backup_dir': os.getenv('BACKUP_DIR', '/var/backups/db'),
        's3_enabled': os.getenv('S3_BACKUP_ENABLED', 'false').lower() == 'true',
        's3_access_key': os.getenv('S3_ACCESS_KEY', ''),
        's3_secret_key': os.getenv('S3_SECRET_KEY', ''),
        's3_bucket': os.getenv('S3_BACKUP_BUCKET', ''),
        's3_region': os.getenv('S3_REGION', 'us-east-1')
    }

# ä½¿ç”¨ç¤ºä¾‹
def setup_backup_system():
    """è®¾ç½®å¤‡ä»½ç³»ç»Ÿ"""
    config = get_backup_config()
    backup_manager = DatabaseBackupManager(config)
    scheduler = BackupScheduler(backup_manager)
    
    # æ·»åŠ å¤‡ä»½è®¡åˆ’
    scheduler.add_schedule('daily', 'daily', 'full')
    scheduler.add_schedule('weekly', 'weekly', 'full')
    
    return backup_manager, scheduler
```

### 4.2 åº”ç”¨çŠ¶æ€å¤‡ä»½

```python
class ApplicationStateBackup:
    """åº”ç”¨çŠ¶æ€å¤‡ä»½"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.backup_dir = Path(config.get('state_backup_dir', '/var/backups/app'))
        self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    def backup_configuration(self) -> Dict[str, Any]:
        """å¤‡ä»½é…ç½®æ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"config_backup_{timestamp}"
        backup_file = self.backup_dir / f"{backup_name}.tar.gz"
        
        try:
            with tarfile.open(backup_file, 'w:gz') as tar:
                # å¤‡ä»½é…ç½®ç›®å½•
                config_dirs = self.config.get('config_dirs', ['/etc/myapp', '/opt/myapp/config'])
                for config_dir in config_dirs:
                    if Path(config_dir).exists():
                        tar.add(config_dir, arcname=Path(config_dir).name)
                
                # å¤‡ä»½ç¯å¢ƒå˜é‡æ–‡ä»¶
                env_files = self.config.get('env_files', ['.env', '.env.production'])
                for env_file in env_files:
                    if Path(env_file).exists():
                        tar.add(env_file)
            
            return {
                'status': 'success',
                'backup_file': str(backup_file),
                'size_bytes': backup_file.stat().st_size,
                'timestamp': timestamp
            }
            
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e),
                'timestamp': timestamp
            }
    
    def backup_logs(self, days: int = 7) -> Dict[str, Any]:
        """å¤‡ä»½æ—¥å¿—æ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"logs_backup_{timestamp}"
        backup_file = self.backup_dir / f"{backup_name}.tar.gz"
        
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            
            with tarfile.open(backup_file, 'w:gz') as tar:
                log_dirs = self.config.get('log_dirs', ['/var/log/myapp', 'logs'])
                
                for log_dir in log_dirs:
                    log_path = Path(log_dir)
                    if log_path.exists():
                        for log_file in log_path.rglob('*.log*'):
                            if log_file.is_file():
                                stat = log_file.stat()
                                modified_time = datetime.fromtimestamp(stat.st_mtime)
                                
                                if modified_time >= cutoff_date:
                                    tar.add(log_file, arcname=f"logs/{log_file.name}")
            
            return {
                'status': 'success',
                'backup_file': str(backup_file),
                'size_bytes': backup_file.stat().st_size,
                'days_included': days,
                'timestamp': timestamp
            }
            
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e),
                'timestamp': timestamp
            }
    
    def create_full_backup(self) -> Dict[str, Any]:
        """åˆ›å»ºå®Œæ•´åº”ç”¨å¤‡ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"full_app_backup_{timestamp}"
        backup_file = self.backup_dir / f"{backup_name}.tar.gz"
        
        try:
            with tarfile.open(backup_file, 'w:gz') as tar:
                # åº”ç”¨ä»£ç 
                app_dirs = self.config.get('app_dirs', ['/opt/myapp'])
                for app_dir in app_dirs:
                    if Path(app_dir).exists():
                        tar.add(app_dir, arcname=f"app/{Path(app_dir).name}")
                
                # é…ç½®æ–‡ä»¶
                config_dirs = self.config.get('config_dirs', [])
                for config_dir in config_dirs:
                    if Path(config_dir).exists():
                        tar.add(config_dir, arcname=f"config/{Path(config_dir).name}")
                
                # æ•°æ®ç›®å½•
                data_dirs = self.config.get('data_dirs', [])
                for data_dir in data_dirs:
                    if Path(data_dir).exists():
                        tar.add(data_dir, arcname=f"data/{Path(data_dir).name}")
            
            return {
                'status': 'success',
                'backup_file': str(backup_file),
                'size_bytes': backup_file.stat().st_size,
                'size_mb': round(backup_file.stat().st_size / 1024 / 1024, 2),
                'timestamp': timestamp
            }
            
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e),
                'timestamp': timestamp
             }
```

## 5. å®¹å™¨åŒ–éƒ¨ç½²

### 5.1 Docker é…ç½®

```dockerfile
# Dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºé root ç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®æƒé™
RUN chown -R appuser:appuser /app
USER appuser

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/myapp
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=INFO
    depends_on:
      - db
      - redis
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    networks:
      - app-network

  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=myapp
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - app
    restart: unless-stopped
    networks:
      - app-network

volumes:
  postgres_data:

networks:
  app-network:
    driver: bridge
```

### 5.2 Kubernetes éƒ¨ç½²

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: myapp
---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: myapp
data:
  LOG_LEVEL: "INFO"
  REDIS_URL: "redis://redis-service:6379"
  DATABASE_HOST: "postgres-service"
  DATABASE_PORT: "5432"
  DATABASE_NAME: "myapp"
---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
  namespace: myapp
type: Opaque
data:
  DATABASE_PASSWORD: cGFzc3dvcmQ=  # base64 encoded 'password'
  SECRET_KEY: c2VjcmV0a2V5  # base64 encoded 'secretkey'
---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
  namespace: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          value: "postgresql://postgres:$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
        envFrom:
        - configMapRef:
            name: app-config
        - secretRef:
            name: app-secrets
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: app-service
  namespace: myapp
spec:
  selector:
    app: myapp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: ClusterIP
---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  namespace: myapp
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - myapp.example.com
    secretName: app-tls
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-service
            port:
              number: 80
```

### 5.3 å®¹å™¨åŒ–æœ€ä½³å®è·µ

```python
class ContainerHealthCheck:
    """å®¹å™¨å¥åº·æ£€æŸ¥"""
    
    def __init__(self, engine):
        self.engine = engine
        self.start_time = datetime.now()
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
        checks = {
            'database': await self._check_database(),
            'redis': await self._check_redis(),
            'disk_space': self._check_disk_space(),
            'memory': self._check_memory()
        }
        
        all_healthy = all(check['status'] == 'healthy' for check in checks.values())
        
        return {
            'status': 'healthy' if all_healthy else 'unhealthy',
            'timestamp': datetime.now().isoformat(),
            'uptime_seconds': (datetime.now() - self.start_time).total_seconds(),
            'checks': checks
        }
    
    async def _check_database(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        try:
            async with self.engine.begin() as conn:
                await conn.execute(text("SELECT 1"))
            return {'status': 'healthy', 'message': 'Database connection OK'}
        except Exception as e:
            return {'status': 'unhealthy', 'message': f'Database error: {str(e)}'}
    
    async def _check_redis(self) -> Dict[str, Any]:
        """æ£€æŸ¥ Redis è¿æ¥"""
        try:
            import redis.asyncio as redis
            redis_client = redis.from_url(os.getenv('REDIS_URL', 'redis://localhost:6379'))
            await redis_client.ping()
            await redis_client.close()
            return {'status': 'healthy', 'message': 'Redis connection OK'}
        except Exception as e:
            return {'status': 'unhealthy', 'message': f'Redis error: {str(e)}'}
    
    def _check_disk_space(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
        try:
            import shutil
            total, used, free = shutil.disk_usage('/')
            free_percent = (free / total) * 100
            
            if free_percent < 10:
                status = 'unhealthy'
                message = f'Low disk space: {free_percent:.1f}% free'
            elif free_percent < 20:
                status = 'warning'
                message = f'Disk space warning: {free_percent:.1f}% free'
            else:
                status = 'healthy'
                message = f'Disk space OK: {free_percent:.1f}% free'
            
            return {
                'status': status,
                'message': message,
                'free_percent': round(free_percent, 1),
                'free_gb': round(free / (1024**3), 2)
            }
        except Exception as e:
            return {'status': 'unhealthy', 'message': f'Disk check error: {str(e)}'}
    
    def _check_memory(self) -> Dict[str, Any]:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            
            if memory.percent > 90:
                status = 'unhealthy'
                message = f'High memory usage: {memory.percent:.1f}%'
            elif memory.percent > 80:
                status = 'warning'
                message = f'Memory usage warning: {memory.percent:.1f}%'
            else:
                status = 'healthy'
                message = f'Memory usage OK: {memory.percent:.1f}%'
            
            return {
                'status': status,
                'message': message,
                'used_percent': round(memory.percent, 1),
                'available_gb': round(memory.available / (1024**3), 2)
            }
        except Exception as e:
            return {'status': 'unhealthy', 'message': f'Memory check error: {str(e)}'}

class GracefulShutdown:
    """ä¼˜é›…å…³é—­å¤„ç†"""
    
    def __init__(self):
        self.shutdown_event = asyncio.Event()
        self.tasks = set()
        self.cleanup_functions = []
    
    def add_cleanup_function(self, func):
        """æ·»åŠ æ¸…ç†å‡½æ•°"""
        self.cleanup_functions.append(func)
    
    def signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
        self.shutdown_event.set()
    
    async def wait_for_shutdown(self):
        """ç­‰å¾…å…³é—­ä¿¡å·"""
        await self.shutdown_event.wait()
        
        print("å¼€å§‹æ¸…ç†èµ„æº...")
        
        # åœæ­¢æ¥å—æ–°ä»»åŠ¡
        for task in self.tasks:
            if not task.done():
                task.cancel()
        
        # ç­‰å¾…ç°æœ‰ä»»åŠ¡å®Œæˆ
        if self.tasks:
            await asyncio.gather(*self.tasks, return_exceptions=True)
        
        # æ‰§è¡Œæ¸…ç†å‡½æ•°
        for cleanup_func in self.cleanup_functions:
            try:
                if asyncio.iscoroutinefunction(cleanup_func):
                    await cleanup_func()
                else:
                    cleanup_func()
            except Exception as e:
                print(f"æ¸…ç†å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        
        print("ä¼˜é›…å…³é—­å®Œæˆ")

# FastAPI åº”ç”¨é›†æˆ
from fastapi import FastAPI
import signal

def create_production_app() -> FastAPI:
    """åˆ›å»ºç”Ÿäº§ç¯å¢ƒåº”ç”¨"""
    app = FastAPI(
        title="MyApp",
        description="ç”Ÿäº§ç¯å¢ƒåº”ç”¨",
        version="1.0.0",
        docs_url=None if os.getenv('ENVIRONMENT') == 'production' else '/docs',
        redoc_url=None if os.getenv('ENVIRONMENT') == 'production' else '/redoc'
    )
    
    # å¥åº·æ£€æŸ¥
    health_checker = ContainerHealthCheck(engine)
    
    @app.get('/health')
    async def health():
        return await health_checker.health_check()
    
    @app.get('/ready')
    async def ready():
        # ç®€å•çš„å°±ç»ªæ£€æŸ¥
        return {'status': 'ready', 'timestamp': datetime.now().isoformat()}
    
    # ä¼˜é›…å…³é—­
    shutdown_handler = GracefulShutdown()
    
    @app.on_event('startup')
    async def startup():
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        signal.signal(signal.SIGTERM, shutdown_handler.signal_handler)
        signal.signal(signal.SIGINT, shutdown_handler.signal_handler)
        
        # æ·»åŠ æ¸…ç†å‡½æ•°
        shutdown_handler.add_cleanup_function(lambda: engine.dispose())
    
    @app.on_event('shutdown')
    async def shutdown():
        await shutdown_handler.wait_for_shutdown()
    
    return app
```

## 6. è´Ÿè½½å‡è¡¡ä¸é«˜å¯ç”¨

### 6.1 Nginx è´Ÿè½½å‡è¡¡é…ç½®

```nginx
# nginx.conf
upstream app_servers {
    least_conn;
    server app1:8000 max_fails=3 fail_timeout=30s;
    server app2:8000 max_fails=3 fail_timeout=30s;
    server app3:8000 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name myapp.example.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name myapp.example.com;
    
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    ssl_prefer_server_ciphers off;
    
    # å®‰å…¨å¤´
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";
    
    # é™æµ
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req zone=api burst=20 nodelay;
    
    # å¥åº·æ£€æŸ¥
    location /health {
        access_log off;
        proxy_pass http://app_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
    
    # API è·¯ç”±
    location /api/ {
        proxy_pass http://app_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
        
        # ç¼“å†²è®¾ç½®
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
    }
    
    # é™æ€æ–‡ä»¶
    location /static/ {
        alias /var/www/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
    
    # é»˜è®¤è·¯ç”±
    location / {
        proxy_pass http://app_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

### 6.2 æ•°æ®åº“é«˜å¯ç”¨

```python
class DatabaseClusterManager:
    """æ•°æ®åº“é›†ç¾¤ç®¡ç†"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.primary_engine = None
        self.replica_engines = []
        self.current_replica_index = 0
        self.setup_connections()
    
    def setup_connections(self):
        """è®¾ç½®æ•°æ®åº“è¿æ¥"""
        # ä¸»åº“è¿æ¥
        primary_url = self.config['primary_url']
        self.primary_engine = create_async_engine(
            primary_url,
            pool_size=self.config.get('pool_size', 20),
            max_overflow=self.config.get('max_overflow', 30),
            pool_timeout=self.config.get('pool_timeout', 30),
            pool_recycle=self.config.get('pool_recycle', 3600)
        )
        
        # ä»åº“è¿æ¥
        for replica_url in self.config.get('replica_urls', []):
            replica_engine = create_async_engine(
                replica_url,
                pool_size=self.config.get('replica_pool_size', 10),
                max_overflow=self.config.get('replica_max_overflow', 20)
            )
            self.replica_engines.append(replica_engine)
    
    def get_write_engine(self):
        """è·å–å†™å…¥å¼•æ“ï¼ˆä¸»åº“ï¼‰"""
        return self.primary_engine
    
    def get_read_engine(self):
        """è·å–è¯»å–å¼•æ“ï¼ˆä»åº“è½®è¯¢ï¼‰"""
        if not self.replica_engines:
            return self.primary_engine
        
        engine = self.replica_engines[self.current_replica_index]
        self.current_replica_index = (self.current_replica_index + 1) % len(self.replica_engines)
        return engine
    
    async def health_check(self) -> Dict[str, Any]:
        """é›†ç¾¤å¥åº·æ£€æŸ¥"""
        results = {
            'primary': await self._check_engine_health(self.primary_engine, 'primary'),
            'replicas': []
        }
        
        for i, engine in enumerate(self.replica_engines):
            replica_health = await self._check_engine_health(engine, f'replica_{i}')
            results['replicas'].append(replica_health)
        
        return results
    
    async def _check_engine_health(self, engine, name: str) -> Dict[str, Any]:
        """æ£€æŸ¥å•ä¸ªå¼•æ“å¥åº·çŠ¶æ€"""
        try:
            async with engine.begin() as conn:
                result = await conn.execute(text("SELECT 1"))
                await result.fetchone()
            
            return {
                'name': name,
                'status': 'healthy',
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            return {
                'name': name,
                'status': 'unhealthy',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

class ReadWriteSession:
    """è¯»å†™åˆ†ç¦»ä¼šè¯"""
    
    def __init__(self, cluster_manager: DatabaseClusterManager):
        self.cluster_manager = cluster_manager
        self._write_session = None
        self._read_session = None
    
    async def get_write_session(self):
        """è·å–å†™å…¥ä¼šè¯"""
        if self._write_session is None:
            engine = self.cluster_manager.get_write_engine()
            self._write_session = async_sessionmaker(engine, expire_on_commit=False)
        return self._write_session()
    
    async def get_read_session(self):
        """è·å–è¯»å–ä¼šè¯"""
        if self._read_session is None:
            engine = self.cluster_manager.get_read_engine()
            self._read_session = async_sessionmaker(engine, expire_on_commit=False)
        return self._read_session()
    
    async def close(self):
        """å…³é—­ä¼šè¯"""
        if self._write_session:
            await self._write_session.close()
        if self._read_session:
            await self._read_session.close()

# ä½¿ç”¨ç¤ºä¾‹
class UserService:
    """ç”¨æˆ·æœåŠ¡ï¼ˆè¯»å†™åˆ†ç¦»ç¤ºä¾‹ï¼‰"""
    
    def __init__(self, cluster_manager: DatabaseClusterManager):
        self.cluster_manager = cluster_manager
    
    async def create_user(self, user_data: Dict[str, Any]) -> User:
        """åˆ›å»ºç”¨æˆ·ï¼ˆå†™æ“ä½œï¼‰"""
        session_manager = ReadWriteSession(self.cluster_manager)
        async with await session_manager.get_write_session() as session:
            user = User(**user_data)
            session.add(user)
            await session.commit()
            await session.refresh(user)
            return user
    
    async def get_user(self, user_id: int) -> Optional[User]:
        """è·å–ç”¨æˆ·ï¼ˆè¯»æ“ä½œï¼‰"""
        session_manager = ReadWriteSession(self.cluster_manager)
        async with await session_manager.get_read_session() as session:
            result = await session.execute(
                select(User).where(User.id == user_id)
            )
            return result.scalar_one_or_none()
    
    async def list_users(self, limit: int = 100, offset: int = 0) -> List[User]:
        """åˆ—å‡ºç”¨æˆ·ï¼ˆè¯»æ“ä½œï¼‰"""
        session_manager = ReadWriteSession(self.cluster_manager)
        async with await session_manager.get_read_session() as session:
            result = await session.execute(
                select(User).limit(limit).offset(offset)
            )
            return result.scalars().all()
```

### 6.3 æ•…éšœè½¬ç§»ä¸è‡ªåŠ¨æ¢å¤

```python
class FailoverManager:
    """æ•…éšœè½¬ç§»ç®¡ç†å™¨"""
    
    def __init__(self, cluster_manager: DatabaseClusterManager):
        self.cluster_manager = cluster_manager
        self.circuit_breaker = CircuitBreaker()
        self.health_check_interval = 30  # ç§’
        self.failover_threshold = 3  # è¿ç»­å¤±è´¥æ¬¡æ•°
        self.recovery_check_interval = 60  # ç§’
        self.failed_engines = set()
        self.monitoring_task = None
    
    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring_task = asyncio.create_task(self._monitor_health())
    
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        if self.monitoring_task:
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass
    
    async def _monitor_health(self):
        """ç›‘æ§å¥åº·çŠ¶æ€"""
        while True:
            try:
                await self._check_all_engines()
                await asyncio.sleep(self.health_check_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
                await asyncio.sleep(self.health_check_interval)
    
    async def _check_all_engines(self):
        """æ£€æŸ¥æ‰€æœ‰å¼•æ“"""
        # æ£€æŸ¥ä¸»åº“
        primary_healthy = await self._check_engine_health(
            self.cluster_manager.primary_engine, 'primary'
        )
        
        if not primary_healthy:
            await self._handle_primary_failure()
        
        # æ£€æŸ¥ä»åº“
        for i, engine in enumerate(self.cluster_manager.replica_engines):
            replica_healthy = await self._check_engine_health(engine, f'replica_{i}')
            if not replica_healthy:
                await self._handle_replica_failure(engine, i)
    
    async def _check_engine_health(self, engine, name: str) -> bool:
        """æ£€æŸ¥å¼•æ“å¥åº·çŠ¶æ€"""
        try:
            async with engine.begin() as conn:
                await conn.execute(text("SELECT 1"))
            return True
        except Exception as e:
            logger.warning(f"å¼•æ“ {name} å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def _handle_primary_failure(self):
        """å¤„ç†ä¸»åº“æ•…éšœ"""
        logger.error("ä¸»åº“æ•…éšœï¼Œå°è¯•æ•…éšœè½¬ç§»")
        
        # é€‰æ‹©æœ€å¥åº·çš„ä»åº“ä½œä¸ºæ–°ä¸»åº“
        best_replica = await self._select_best_replica()
        if best_replica:
            await self._promote_replica_to_primary(best_replica)
        else:
            logger.critical("æ²¡æœ‰å¯ç”¨çš„ä»åº“è¿›è¡Œæ•…éšœè½¬ç§»")
            # å‘é€ç´§æ€¥å‘Šè­¦
            await self._send_critical_alert("æ•°æ®åº“å®Œå…¨ä¸å¯ç”¨")
    
    async def _handle_replica_failure(self, engine, index: int):
        """å¤„ç†ä»åº“æ•…éšœ"""
        logger.warning(f"ä»åº“ {index} æ•…éšœï¼Œä»è´Ÿè½½å‡è¡¡ä¸­ç§»é™¤")
        self.failed_engines.add(engine)
        
        # ä»é›†ç¾¤ä¸­ä¸´æ—¶ç§»é™¤æ•…éšœä»åº“
        if engine in self.cluster_manager.replica_engines:
            self.cluster_manager.replica_engines.remove(engine)
    
    async def _select_best_replica(self):
        """é€‰æ‹©æœ€ä½³ä»åº“"""
        best_replica = None
        best_lag = float('inf')
        
        for engine in self.cluster_manager.replica_engines:
            try:
                lag = await self._get_replication_lag(engine)
                if lag < best_lag:
                    best_lag = lag
                    best_replica = engine
            except Exception as e:
                logger.warning(f"æ— æ³•è·å–ä»åº“å»¶è¿Ÿ: {e}")
        
        return best_replica
    
    async def _get_replication_lag(self, engine) -> float:
        """è·å–å¤åˆ¶å»¶è¿Ÿ"""
        async with engine.begin() as conn:
            result = await conn.execute(text(
                "SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))"
            ))
            lag = result.scalar()
            return lag if lag is not None else 0.0
    
    async def _promote_replica_to_primary(self, replica_engine):
        """æå‡ä»åº“ä¸ºä¸»åº“"""
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„æ•°æ®åº“é›†ç¾¤æ–¹æ¡ˆå®ç°
            # ä¾‹å¦‚ï¼šPostgreSQL çš„ pg_promote()
            async with replica_engine.begin() as conn:
                await conn.execute(text("SELECT pg_promote()"))
            
            # æ›´æ–°é›†ç¾¤é…ç½®
            old_primary = self.cluster_manager.primary_engine
            self.cluster_manager.primary_engine = replica_engine
            self.cluster_manager.replica_engines.remove(replica_engine)
            
            logger.info("æ•…éšœè½¬ç§»å®Œæˆï¼Œæ–°ä¸»åº“å·²å¯ç”¨")
            
            # å‘é€æ•…éšœè½¬ç§»é€šçŸ¥
            await self._send_failover_notification()
            
        except Exception as e:
            logger.error(f"æ•…éšœè½¬ç§»å¤±è´¥: {e}")
            await self._send_critical_alert(f"æ•…éšœè½¬ç§»å¤±è´¥: {e}")
    
    async def _send_failover_notification(self):
        """å‘é€æ•…éšœè½¬ç§»é€šçŸ¥"""
        # å®ç°å‘Šè­¦é€šçŸ¥é€»è¾‘
        pass
    
    async def _send_critical_alert(self, message: str):
        """å‘é€ç´§æ€¥å‘Šè­¦"""
        # å®ç°ç´§æ€¥å‘Šè­¦é€»è¾‘
        pass

class CircuitBreaker:
    """ç†”æ–­å™¨"""
    
    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
    
    async def call(self, func, *args, **kwargs):
        """é€šè¿‡ç†”æ–­å™¨è°ƒç”¨å‡½æ•°"""
        if self.state == 'OPEN':
            if self._should_attempt_reset():
                self.state = 'HALF_OPEN'
            else:
                raise Exception("ç†”æ–­å™¨å¼€å¯ï¼Œæ‹’ç»è¯·æ±‚")
        
        try:
            result = await func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e
    
    def _should_attempt_reset(self) -> bool:
        """æ˜¯å¦åº”è¯¥å°è¯•é‡ç½®"""
        return (
            self.last_failure_time and
            time.time() - self.last_failure_time >= self.recovery_timeout
        )
    
    def _on_success(self):
        """æˆåŠŸæ—¶çš„å¤„ç†"""
        self.failure_count = 0
        self.state = 'CLOSED'
    
    def _on_failure(self):
        """å¤±è´¥æ—¶çš„å¤„ç†"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'
```

## 7. è‡ªåŠ¨æ‰©ç¼©å®¹

### 7.1 æ°´å¹³æ‰©ç¼©å®¹

```python
class AutoScaler:
    """è‡ªåŠ¨æ‰©ç¼©å®¹ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.min_replicas = config.get('min_replicas', 2)
        self.max_replicas = config.get('max_replicas', 10)
        self.target_cpu_utilization = config.get('target_cpu_utilization', 70)
        self.target_memory_utilization = config.get('target_memory_utilization', 80)
        self.scale_up_threshold = config.get('scale_up_threshold', 80)
        self.scale_down_threshold = config.get('scale_down_threshold', 30)
        self.cooldown_period = config.get('cooldown_period', 300)  # 5åˆ†é’Ÿ
        self.last_scale_time = 0
        self.metrics_collector = MetricsCollector()
    
    async def start_autoscaling(self):
        """å¼€å§‹è‡ªåŠ¨æ‰©ç¼©å®¹"""
        while True:
            try:
                await self._check_and_scale()
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                logger.error(f"è‡ªåŠ¨æ‰©ç¼©å®¹æ£€æŸ¥å¤±è´¥: {e}")
                await asyncio.sleep(60)
    
    async def _check_and_scale(self):
        """æ£€æŸ¥å¹¶æ‰§è¡Œæ‰©ç¼©å®¹"""
        if not self._can_scale():
            return
        
        current_metrics = await self._get_current_metrics()
        current_replicas = await self._get_current_replicas()
        
        # è®¡ç®—ç›®æ ‡å‰¯æœ¬æ•°
        target_replicas = self._calculate_target_replicas(
            current_metrics, current_replicas
        )
        
        if target_replicas != current_replicas:
            await self._scale_to(target_replicas)
            self.last_scale_time = time.time()
    
    def _can_scale(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰©ç¼©å®¹ï¼ˆå†·å´æœŸæ£€æŸ¥ï¼‰"""
        return time.time() - self.last_scale_time >= self.cooldown_period
    
    async def _get_current_metrics(self) -> Dict[str, float]:
        """è·å–å½“å‰æŒ‡æ ‡"""
        return {
            'cpu_utilization': await self.metrics_collector.get_avg_cpu_utilization(),
            'memory_utilization': await self.metrics_collector.get_avg_memory_utilization(),
            'request_rate': await self.metrics_collector.get_request_rate(),
            'response_time': await self.metrics_collector.get_avg_response_time()
        }
    
    async def _get_current_replicas(self) -> int:
        """è·å–å½“å‰å‰¯æœ¬æ•°"""
        # è¿™é‡Œéœ€è¦æ ¹æ®éƒ¨ç½²å¹³å°å®ç°
        # Kubernetes ç¤ºä¾‹
        try:
            import kubernetes
            v1 = kubernetes.client.AppsV1Api()
            deployment = v1.read_namespaced_deployment(
                name=self.config['deployment_name'],
                namespace=self.config['namespace']
            )
            return deployment.spec.replicas
        except Exception as e:
            logger.error(f"è·å–å½“å‰å‰¯æœ¬æ•°å¤±è´¥: {e}")
            return self.min_replicas
    
    def _calculate_target_replicas(self, metrics: Dict[str, float], current_replicas: int) -> int:
        """è®¡ç®—ç›®æ ‡å‰¯æœ¬æ•°"""
        cpu_ratio = metrics['cpu_utilization'] / self.target_cpu_utilization
        memory_ratio = metrics['memory_utilization'] / self.target_memory_utilization
        
        # ä½¿ç”¨æœ€é«˜çš„èµ„æºåˆ©ç”¨ç‡æ¥è®¡ç®—
        max_ratio = max(cpu_ratio, memory_ratio)
        
        target_replicas = int(current_replicas * max_ratio)
        
        # åº”ç”¨æœ€å°å’Œæœ€å¤§é™åˆ¶
        target_replicas = max(self.min_replicas, min(self.max_replicas, target_replicas))
        
        # é¿å…é¢‘ç¹çš„å°å¹…è°ƒæ•´
        if abs(target_replicas - current_replicas) == 1:
            if metrics['cpu_utilization'] < self.scale_down_threshold and \
               metrics['memory_utilization'] < self.scale_down_threshold:
                target_replicas = current_replicas - 1
            elif metrics['cpu_utilization'] > self.scale_up_threshold or \
                 metrics['memory_utilization'] > self.scale_up_threshold:
                target_replicas = current_replicas + 1
            else:
                target_replicas = current_replicas
        
        return target_replicas
    
    async def _scale_to(self, target_replicas: int):
        """æ‰©ç¼©å®¹åˆ°ç›®æ ‡å‰¯æœ¬æ•°"""
        try:
            import kubernetes
            v1 = kubernetes.client.AppsV1Api()
            
            # æ›´æ–° Deployment
            body = {'spec': {'replicas': target_replicas}}
            v1.patch_namespaced_deployment(
                name=self.config['deployment_name'],
                namespace=self.config['namespace'],
                body=body
            )
            
            logger.info(f"æ‰©ç¼©å®¹å®Œæˆ: ç›®æ ‡å‰¯æœ¬æ•° {target_replicas}")
            
            # è®°å½•æ‰©ç¼©å®¹äº‹ä»¶
            await self._record_scaling_event(target_replicas)
            
        except Exception as e:
            logger.error(f"æ‰©ç¼©å®¹å¤±è´¥: {e}")
    
    async def _record_scaling_event(self, target_replicas: int):
        """è®°å½•æ‰©ç¼©å®¹äº‹ä»¶"""
        event = {
            'timestamp': datetime.now().isoformat(),
            'event_type': 'scaling',
            'target_replicas': target_replicas,
            'metrics': await self._get_current_metrics()
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“æˆ–å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
        logger.info(f"æ‰©ç¼©å®¹äº‹ä»¶: {event}")

class ResourceOptimizer:
    """èµ„æºä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimization_history = []
    
    async def optimize_resources(self) -> Dict[str, Any]:
        """ä¼˜åŒ–èµ„æºé…ç½®"""
        recommendations = {
            'cpu': await self._optimize_cpu(),
            'memory': await self._optimize_memory(),
            'storage': await self._optimize_storage(),
            'network': await self._optimize_network()
        }
        
        return recommendations
    
    async def _optimize_cpu(self) -> Dict[str, Any]:
        """CPU ä¼˜åŒ–å»ºè®®"""
        # åˆ†æ CPU ä½¿ç”¨æ¨¡å¼
        cpu_metrics = await self._get_cpu_metrics()
        
        recommendations = []
        
        if cpu_metrics['avg_utilization'] < 30:
            recommendations.append({
                'type': 'reduce_cpu',
                'current': cpu_metrics['allocated'],
                'recommended': cpu_metrics['allocated'] * 0.8,
                'reason': 'CPU åˆ©ç”¨ç‡è¿‡ä½'
            })
        elif cpu_metrics['avg_utilization'] > 80:
            recommendations.append({
                'type': 'increase_cpu',
                'current': cpu_metrics['allocated'],
                'recommended': cpu_metrics['allocated'] * 1.2,
                'reason': 'CPU åˆ©ç”¨ç‡è¿‡é«˜'
            })
        
        return {
            'current_metrics': cpu_metrics,
            'recommendations': recommendations
        }
    
    async def _optimize_memory(self) -> Dict[str, Any]:
        """å†…å­˜ä¼˜åŒ–å»ºè®®"""
        memory_metrics = await self._get_memory_metrics()
        
        recommendations = []
        
        if memory_metrics['avg_utilization'] < 40:
            recommendations.append({
                'type': 'reduce_memory',
                'current': memory_metrics['allocated'],
                'recommended': memory_metrics['allocated'] * 0.8,
                'reason': 'å†…å­˜åˆ©ç”¨ç‡è¿‡ä½'
            })
        elif memory_metrics['avg_utilization'] > 85:
            recommendations.append({
                'type': 'increase_memory',
                'current': memory_metrics['allocated'],
                'recommended': memory_metrics['allocated'] * 1.3,
                'reason': 'å†…å­˜åˆ©ç”¨ç‡è¿‡é«˜'
            })
        
        return {
            'current_metrics': memory_metrics,
            'recommendations': recommendations
        }
    
    async def _get_cpu_metrics(self) -> Dict[str, float]:
        """è·å– CPU æŒ‡æ ‡"""
        # å®ç° CPU æŒ‡æ ‡æ”¶é›†
        return {
            'avg_utilization': 45.0,
            'peak_utilization': 78.0,
            'allocated': 2.0  # CPU æ ¸å¿ƒæ•°
        }
    
    async def _get_memory_metrics(self) -> Dict[str, float]:
        """è·å–å†…å­˜æŒ‡æ ‡"""
        # å®ç°å†…å­˜æŒ‡æ ‡æ”¶é›†
        return {
            'avg_utilization': 65.0,
            'peak_utilization': 89.0,
            'allocated': 4096  # MB
        }
    
    async def _optimize_storage(self) -> Dict[str, Any]:
        """å­˜å‚¨ä¼˜åŒ–å»ºè®®"""
        return {'recommendations': []}
    
    async def _optimize_network(self) -> Dict[str, Any]:
        """ç½‘ç»œä¼˜åŒ–å»ºè®®"""
        return {'recommendations': []}
```

## 8. æœ€ä½³å®è·µ

### 8.1 éƒ¨ç½²æœ€ä½³å®è·µ

```python
class DeploymentBestPractices:
    """éƒ¨ç½²æœ€ä½³å®è·µ"""
    
    @staticmethod
    def get_deployment_checklist() -> List[Dict[str, Any]]:
        """è·å–éƒ¨ç½²æ£€æŸ¥æ¸…å•"""
        return [
            {
                'category': 'å®‰å…¨é…ç½®',
                'items': [
                    'æ‰€æœ‰æ•æ„Ÿä¿¡æ¯ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–å¯†é’¥ç®¡ç†',
                    'å¯ç”¨ HTTPS å’Œ TLS åŠ å¯†',
                    'é…ç½®é˜²ç«å¢™å’Œç½‘ç»œå®‰å…¨ç»„',
                    'å®šæœŸæ›´æ–°ä¾èµ–åŒ…å’ŒåŸºç¡€é•œåƒ',
                    'å®æ–½æœ€å°æƒé™åŸåˆ™'
                ]
            },
            {
                'category': 'æ€§èƒ½ä¼˜åŒ–',
                'items': [
                    'é…ç½®é€‚å½“çš„è¿æ¥æ± å¤§å°',
                    'å¯ç”¨æ•°æ®åº“æŸ¥è¯¢ç¼“å­˜',
                    'å®æ–½ CDN å’Œé™æ€èµ„æºç¼“å­˜',
                    'ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•',
                    'é…ç½®è´Ÿè½½å‡è¡¡'
                ]
            },
            {
                'category': 'ç›‘æ§å‘Šè­¦',
                'items': [
                    'é…ç½®åº”ç”¨æ€§èƒ½ç›‘æ§',
                    'è®¾ç½®å…³é”®æŒ‡æ ‡å‘Šè­¦',
                    'å®æ–½æ—¥å¿—èšåˆå’Œåˆ†æ',
                    'é…ç½®å¥åº·æ£€æŸ¥ç«¯ç‚¹',
                    'ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ'
                ]
            },
            {
                'category': 'å¯é æ€§',
                'items': [
                    'å®æ–½å¤šå‰¯æœ¬éƒ¨ç½²',
                    'é…ç½®è‡ªåŠ¨æ•…éšœè½¬ç§»',
                    'å®šæœŸå¤‡ä»½æ•°æ®',
                    'å®æ–½ä¼˜é›…å…³é—­',
                    'é…ç½®ç†”æ–­å™¨å’Œé‡è¯•æœºåˆ¶'
                ]
            }
        ]
    
    @staticmethod
    def validate_production_config(config: Dict[str, Any]) -> List[str]:
        """éªŒè¯ç”Ÿäº§ç¯å¢ƒé…ç½®"""
        issues = []
        
        # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
        required_vars = [
            'DATABASE_URL', 'SECRET_KEY', 'LOG_LEVEL'
        ]
        
        for var in required_vars:
            if not config.get(var):
                issues.append(f"ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {var}")
        
        # æ£€æŸ¥å®‰å…¨é…ç½®
        if config.get('DEBUG', False):
            issues.append("ç”Ÿäº§ç¯å¢ƒä¸åº”å¯ç”¨ DEBUG æ¨¡å¼")
        
        if not config.get('SECRET_KEY') or len(config.get('SECRET_KEY', '')) < 32:
            issues.append("SECRET_KEY é•¿åº¦åº”è‡³å°‘ä¸º 32 ä¸ªå­—ç¬¦")
        
        # æ£€æŸ¥æ•°æ®åº“é…ç½®
        db_url = config.get('DATABASE_URL', '')
        if 'localhost' in db_url or '127.0.0.1' in db_url:
            issues.append("ç”Ÿäº§ç¯å¢ƒä¸åº”ä½¿ç”¨æœ¬åœ°æ•°æ®åº“")
        
        return issues
    
    @staticmethod
    def generate_deployment_script(config: Dict[str, Any]) -> str:
        """ç”Ÿæˆéƒ¨ç½²è„šæœ¬"""
        script = f"""
#!/bin/bash
set -e

# ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è„šæœ¬
echo "å¼€å§‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ..."

# 1. æ‹‰å–æœ€æ–°ä»£ç 
git pull origin main

# 2. æ„å»º Docker é•œåƒ
docker build -t {config.get('app_name', 'myapp')}:latest .

# 3. è¿è¡Œæ•°æ®åº“è¿ç§»
docker run --rm \
  --env-file .env.production \
  {config.get('app_name', 'myapp')}:latest \
  alembic upgrade head

# 4. æ»šåŠ¨æ›´æ–°åº”ç”¨
docker-compose -f docker-compose.prod.yml up -d --no-deps app

# 5. å¥åº·æ£€æŸ¥
echo "ç­‰å¾…åº”ç”¨å¯åŠ¨..."
sleep 30

if curl -f http://localhost:8000/health; then
    echo "éƒ¨ç½²æˆåŠŸï¼"
else
    echo "éƒ¨ç½²å¤±è´¥ï¼Œå›æ»š..."
    docker-compose -f docker-compose.prod.yml rollback
    exit 1
fi

echo "éƒ¨ç½²å®Œæˆ"
"""
        return script

class ProductionMonitoring:
    """ç”Ÿäº§ç¯å¢ƒç›‘æ§"""
    
    def __init__(self):
        self.alerts = []
        self.metrics_history = []
    
    async def setup_monitoring(self):
        """è®¾ç½®ç›‘æ§"""
        # è®¾ç½®å…³é”®æŒ‡æ ‡ç›‘æ§
        await self._setup_application_monitoring()
        await self._setup_infrastructure_monitoring()
        await self._setup_business_monitoring()
    
    async def _setup_application_monitoring(self):
        """è®¾ç½®åº”ç”¨ç›‘æ§"""
        monitors = [
            {
                'name': 'response_time',
                'threshold': 1000,  # ms
                'description': 'API å“åº”æ—¶é—´ç›‘æ§'
            },
            {
                'name': 'error_rate',
                'threshold': 5,  # %
                'description': 'é”™è¯¯ç‡ç›‘æ§'
            },
            {
                'name': 'throughput',
                'threshold': 100,  # req/s
                'description': 'ååé‡ç›‘æ§'
            }
        ]
        
        for monitor in monitors:
            await self._create_monitor(monitor)
    
    async def _setup_infrastructure_monitoring(self):
        """è®¾ç½®åŸºç¡€è®¾æ–½ç›‘æ§"""
        monitors = [
            {
                'name': 'cpu_utilization',
                'threshold': 80,  # %
                'description': 'CPU ä½¿ç”¨ç‡ç›‘æ§'
            },
            {
                'name': 'memory_utilization',
                'threshold': 85,  # %
                'description': 'å†…å­˜ä½¿ç”¨ç‡ç›‘æ§'
            },
            {
                'name': 'disk_usage',
                'threshold': 90,  # %
                'description': 'ç£ç›˜ä½¿ç”¨ç‡ç›‘æ§'
            },
            {
                'name': 'database_connections',
                'threshold': 80,  # % of pool
                'description': 'æ•°æ®åº“è¿æ¥æ•°ç›‘æ§'
            }
        ]
        
        for monitor in monitors:
            await self._create_monitor(monitor)
    
    async def _setup_business_monitoring(self):
        """è®¾ç½®ä¸šåŠ¡ç›‘æ§"""
        monitors = [
            {
                'name': 'user_registrations',
                'threshold': 10,  # per hour
                'description': 'ç”¨æˆ·æ³¨å†Œæ•°ç›‘æ§'
            },
            {
                'name': 'transaction_volume',
                'threshold': 1000,  # per hour
                'description': 'äº¤æ˜“é‡ç›‘æ§'
            }
        ]
        
        for monitor in monitors:
            await self._create_monitor(monitor)
    
    async def _create_monitor(self, monitor_config: Dict[str, Any]):
        """åˆ›å»ºç›‘æ§å™¨"""
        # å®ç°ç›‘æ§å™¨åˆ›å»ºé€»è¾‘
        logger.info(f"åˆ›å»ºç›‘æ§å™¨: {monitor_config['name']}")
```

## 9. æœ¬ç« æ€»ç»“

### 9.1 æ ¸å¿ƒæ¦‚å¿µå›é¡¾

æœ¬ç« è¯¦ç»†ä»‹ç»äº† SQLModel åº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€é…ç½®ã€ç›‘æ§å’Œè¿ç»´æœ€ä½³å®è·µï¼š

1. **ç”Ÿäº§ç¯å¢ƒé…ç½®**
   - æ•°æ®åº“è¿æ¥æ± é…ç½®å’Œä¼˜åŒ–
   - å®‰å…¨é…ç½®å’Œè®¿é—®æ§åˆ¶
   - ç¯å¢ƒå˜é‡ç®¡ç†

2. **ç›‘æ§ä¸å‘Šè­¦**
   - åº”ç”¨æ€§èƒ½ç›‘æ§
   - ç³»ç»Ÿèµ„æºç›‘æ§
   - å‘Šè­¦ç®¡ç†å’Œé€šçŸ¥

3. **æ—¥å¿—ç®¡ç†**
   - ç»“æ„åŒ–æ—¥å¿—è®°å½•
   - æ—¥å¿—è½®è½¬å’Œå‹ç¼©
   - æ—¥å¿—åˆ†æå’ŒæŸ¥è¯¢

4. **å¤‡ä»½ä¸æ¢å¤**
   - æ•°æ®åº“å¤‡ä»½ç­–ç•¥
   - åº”ç”¨çŠ¶æ€å¤‡ä»½
   - ç¾éš¾æ¢å¤è®¡åˆ’

5. **å®¹å™¨åŒ–éƒ¨ç½²**
   - Docker é…ç½®å’Œä¼˜åŒ–
   - Kubernetes éƒ¨ç½²
   - å¥åº·æ£€æŸ¥å’Œä¼˜é›…å…³é—­

6. **è´Ÿè½½å‡è¡¡ä¸é«˜å¯ç”¨**
   - Nginx è´Ÿè½½å‡è¡¡é…ç½®
   - æ•°æ®åº“è¯»å†™åˆ†ç¦»
   - æ•…éšœè½¬ç§»å’Œè‡ªåŠ¨æ¢å¤

7. **è‡ªåŠ¨æ‰©ç¼©å®¹**
   - æ°´å¹³æ‰©ç¼©å®¹ç­–ç•¥
   - èµ„æºä¼˜åŒ–å»ºè®®
   - æ€§èƒ½è°ƒä¼˜

### 9.2 æœ€ä½³å®è·µæ€»ç»“

1. **å®‰å…¨ç¬¬ä¸€**
   - ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿä¿¡æ¯
   - å®æ–½æœ€å°æƒé™åŸåˆ™
   - å®šæœŸæ›´æ–°ä¾èµ–å’Œå®‰å…¨è¡¥ä¸

2. **ç›‘æ§é©±åŠ¨**
   - å»ºç«‹å…¨é¢çš„ç›‘æ§ä½“ç³»
   - è®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼
   - å®šæœŸå®¡æŸ¥å’Œä¼˜åŒ–ç›‘æ§ç­–ç•¥

3. **è‡ªåŠ¨åŒ–è¿ç»´**
   - è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹
   - è‡ªåŠ¨åŒ–å¤‡ä»½å’Œæ¢å¤
   - è‡ªåŠ¨åŒ–æ‰©ç¼©å®¹å’Œæ•…éšœå¤„ç†

4. **æ€§èƒ½ä¼˜åŒ–**
   - åˆç†é…ç½®è¿æ¥æ± 
   - å®æ–½ç¼“å­˜ç­–ç•¥
   - ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢

### 9.3 å¸¸è§é™·é˜±ä¸é¿å…æ–¹æ³•

1. **é…ç½®ç®¡ç†é™·é˜±**
   - âŒ ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯
   - âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡å’Œå¯†é’¥ç®¡ç†

2. **ç›‘æ§ç›²åŒº**
   - âŒ åªç›‘æ§åŸºç¡€æŒ‡æ ‡
   - âœ… å»ºç«‹ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§

3. **å•ç‚¹æ•…éšœ**
   - âŒ å•å®ä¾‹éƒ¨ç½²
   - âœ… å¤šå‰¯æœ¬å’Œæ•…éšœè½¬ç§»

4. **èµ„æºæµªè´¹**
   - âŒ å›ºå®šèµ„æºé…ç½®
   - âœ… åŠ¨æ€æ‰©ç¼©å®¹å’Œèµ„æºä¼˜åŒ–

### 9.4 å®è·µæ£€æŸ¥æ¸…å•

- [ ] é…ç½®ç”Ÿäº§ç¯å¢ƒæ•°æ®åº“è¿æ¥
- [ ] å®æ–½å®‰å…¨é…ç½®å’Œè®¿é—®æ§åˆ¶
- [ ] è®¾ç½®ç¯å¢ƒå˜é‡ç®¡ç†
- [ ] é…ç½®åº”ç”¨ç›‘æ§å’Œå‘Šè­¦
- [ ] å®æ–½æ—¥å¿—ç®¡ç†ç­–ç•¥
- [ ] å»ºç«‹å¤‡ä»½å’Œæ¢å¤æœºåˆ¶
- [ ] å®¹å™¨åŒ–åº”ç”¨éƒ¨ç½²
- [ ] é…ç½®è´Ÿè½½å‡è¡¡å’Œé«˜å¯ç”¨
- [ ] å®æ–½è‡ªåŠ¨æ‰©ç¼©å®¹
- [ ] å»ºç«‹è¿ç»´æœ€ä½³å®è·µ

### 9.5 ä¸‹ä¸€æ­¥å­¦ä¹ 

1. **æ·±å…¥å­¦ä¹ å®¹å™¨ç¼–æ’**
   - Kubernetes é«˜çº§ç‰¹æ€§
   - æœåŠ¡ç½‘æ ¼ï¼ˆService Meshï¼‰
   - GitOps éƒ¨ç½²ç­–ç•¥

2. **äº‘åŸç”ŸæŠ€æœ¯**
   - å¾®æœåŠ¡æ¶æ„
   - æ— æœåŠ¡å™¨è®¡ç®—
   - äº‘åŸç”Ÿæ•°æ®åº“

3. **DevOps å®è·µ**
   - CI/CD æµæ°´çº¿
   - åŸºç¡€è®¾æ–½å³ä»£ç 
   - æ··æ²Œå·¥ç¨‹

### 9.6 æ‰©å±•ç»ƒä¹ 

1. **éƒ¨ç½²ç»ƒä¹ **
   - åœ¨äº‘å¹³å°éƒ¨ç½²å®Œæ•´çš„ SQLModel åº”ç”¨
   - é…ç½®å¤šç¯å¢ƒéƒ¨ç½²æµç¨‹
   - å®æ–½è“ç»¿éƒ¨ç½²ç­–ç•¥

2. **ç›‘æ§ç»ƒä¹ **
   - é›†æˆ Prometheus å’Œ Grafana
   - é…ç½®è‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡
   - å»ºç«‹å‘Šè­¦å“åº”æµç¨‹

3. **é«˜å¯ç”¨ç»ƒä¹ **
   - é…ç½®æ•°æ®åº“ä¸»ä»å¤åˆ¶
   - å®æ–½åº”ç”¨æ•…éšœè½¬ç§»
   - è¿›è¡Œç¾éš¾æ¢å¤æ¼”ç»ƒ

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»æŒæ¡äº† SQLModel åº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å®Œæ•´éƒ¨ç½²å’Œè¿ç»´çŸ¥è¯†ã€‚è¿™äº›å®è·µå°†å¸®åŠ©ä½ æ„å»ºç¨³å®šã€å¯é ã€é«˜æ€§èƒ½çš„ç”Ÿäº§ç³»ç»Ÿã€‚